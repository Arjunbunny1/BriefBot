{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !capture --no-stderr\n",
    "# !pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"nomic[local]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"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\")\n",
    "_set_env(\"nk-p4RbLXYiBQInfAQyrlCssat_n9w-697uXrq4dlCmq0o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_nomic.embeddings import NomicEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = search_scholar(\"LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for val in res.values():\n",
    "    urls.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/pdf/2309.05519.pdf?trk=article-ssr-frontend-pulse_x-social-details_comments-action_comment-text'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    # \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    # \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define the paths to your local files\n",
    "local_files = [\n",
    "    \"/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf\",\n",
    "]\n",
    "\n",
    "# Class to wrap the file content\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata=None):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata or {}\n",
    "\n",
    "# Function to load the content from local PDF files\n",
    "def load_local_pdf(file_path):\n",
    "    doc = fitz.open(file_path)\n",
    "    content = \"\"\n",
    "    for page in doc:\n",
    "        content += page.get_text()\n",
    "    return Document(content, metadata={\"source\": file_path})\n",
    "\n",
    "# Load the documents from the local files\n",
    "docs = [load_local_pdf(file_path) for file_path in local_files]\n",
    "\n",
    "# Split the documents using the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='NExT-GPT: Any-to-Any Multimodal LLM\\nShengqiong Wu 1 Hao Fei 1 Leigang Qu 1 Wei Ji 1 Tat-Seng Chua 1\\nAbstract\\nWhile recently Multimodal Large Language Mod-\\nels (MM-LLMs) have made exciting strides, they\\nmostly fall prey to the limitation of only input-\\nside multimodal understanding, without the abil-\\nity to produce content in multiple modalities. As\\nwe humans always perceive the world and com-\\nmunicate with people through various modalities,\\ndeveloping any-to-any MM-LLMs capable of ac-\\ncepting and delivering content in any modality\\nbecomes essential to human-level AI. To fill the\\ngap, we present an end-to-end general-purpose\\nany-to-any MM-LLM system, NExT-GPT. We\\nconnect an LLM with multimodal adaptors and\\ndifferent diffusion decoders, enabling NExT-GPT\\nto perceive inputs and generate outputs in arbi-\\ntrary combinations of text, image, video, and\\naudio. By leveraging the existing well-trained\\nhigh-performing encoders and decoders, NExT-\\nGPT is tuned with only a small amount of pa-\\nrameter (1%) of certain projection layers, which\\nnot only benefits low-cost training but also fa-\\ncilitates convenient expansion to more potential\\nmodalities. Moreover, we introduce a modality-\\nswitching instruction tuning (MosIT) and manu-\\nally curate a high-quality dataset for MosIT, based\\non which NExT-GPT is empowered with com-\\nplex cross-modal semantic understanding and con-\\ntent generation. Overall, our research showcases\\nthe promising possibility of building a unified\\nAI agent capable of modeling universal modal-\\nities, paving the way for more human-like AI\\nresearch in the community.\\nProject website:\\nhttps://next-gpt.github.io/\\n1. Introduction\\nRecently, the topic of Artificial Intelligence Generated Con-\\ntent (AIGC) has witnessed unprecedented advancements\\n1NExT++ Research Center, National University of Singapore, Singa-\\npore. Correspondence to: Hao Fei <haofei37@nus.edu.sg>.\\nProceedings of the 41 st International Conference on Machine Learning,\\nVienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s).\\nwith certain technologies, such as ChatGPT for text gen-\\neration (OpenAI, 2022a) and diffusion models for visual\\ngeneration (Fan et al., 2022). Among these, the rise of Large\\nLanguage Models (LLMs) has been particularly remarkable,\\ne.g., Flan-T5 (Chung et al., 2022), Vicuna (Chiang et al.,\\n2023), LLaMA (Touvron et al., 2023) and Alpaca (Taori\\net al., 2023), showcasing their formidable human-level lan-\\nguage reasoning and decision-making capabilities, shining\\na light on the path of Artificial General Intelligence (AGI).\\nOur world is inherently multimodal, and humans perceive\\nthe world with different sensory organs for varied modal\\ninformation, such as language, images, videos, and sounds,\\nwhich often complement and synergize with each other.\\nWith such intuition, the purely text-based LLMs have re-\\ncently been endowed with other modal understanding and\\nperception capabilities of image, video, audio, etc.\\nA notable approach involves employing adapters that align\\npre-trained encoders in other modalities to textual LLMs.\\nThis endeavor has led to the rapid development of mul-\\ntimodal LLMs (MM-LLMs), such as BLIP-2 (Li et al.,\\n2023c), Flamingo (Alayrac et al., 2022), MiniGPT-4 (Zhu\\net al., 2023), Video-LLaMA (Zhang et al., 2023c), LLaVA\\n(Liu et al., 2023b), PandaGPT (Su et al., 2023), and\\nSpeechGPT (Zhang et al., 2023b). Nevertheless, most of\\nthese efforts pay attention to the multimodal content under-\\nstanding at the input side. Lately, fewer works have consid-\\nered multimodal generation, such as Emu (Sun et al., 2023),'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='DREAMLLM (Dong et al., 2023), GILL (Koh et al., 2023),\\nSEED (Ge et al., 2023). Notably, these models are confined\\nto generating interleaved texts and images. We emphasize\\nthat natural human cognition and communication indispens-\\nably require seamless transitions between any modalities\\nof information. This makes the exploration of any-to-any\\nMM-LLMs critical, i.e., the ability to accept inputs in any\\nmodality and deliver responses in any appropriate modality.\\nCertain efforts have been made to mimic the human-like\\nany-to-any modality conversion. Lately, CoDi (Tang et al.,\\n2023) has made strides in implementing the capability of\\nsimultaneously processing and generating arbitrary combi-\\nnations of modalities; however, it lacks the reasoning and\\ndecision-making prowess of LLMs as its core, and is also\\nlimited to simple paired content generation. On the other\\nhand, some efforts, e.g., Visual-ChatGPT (Wu et al., 2023)\\n1\\narXiv:2309.05519v3  [cs.AI]  25 Jun 2024\\nNExT-GPT: Any-to-Any Multimodal LLM\\nLLM\\nAudio \\nDiffusion\\nVideo \\nDiffusion\\nLLM-based Semantic \\nUnderstanding\\nInstruction-following \\nAlignment\\nImage \\nDiffusion \\nText\\nImage\\nAudio\\nVideo\\nImage Output \\nProjection\\nAudio Output \\nProjection\\nVideo Output \\nProjection\\nImage Input \\nProjection\\nImage \\nEncoder\\nAudio Input \\nProjection\\nAudio \\nEncoder\\nVideo \\nEncoder\\nVideo Input \\nProjection\\n...\\nMore modalities\\n...\\nMultimodal Input \\nEncoding \\nMultimodal Output \\nGeneration\\nLLM-centric \\nAlignment\\nFigure 1. By connecting LLM with multimodal adaptors and diffusion decoders, NExT-GPT achieves universal multimodal understanding and any-to-any\\nmodality input and output.\\nand\\nrepresent the frozen and trainable modules, respectively.\\nand HuggingGPT (Shen et al., 2023), have sought to com-\\nbine LLMs with external tools to achieve approximately\\nthe ‘any-to-any’ multimodal understanding and generation.\\nUnfortunately, these systems suffer from critical challenges\\ndue to their complete pipeline architecture. First, the in-\\nformation transfer between different modules is entirely\\nbased on discrete texts produced by the LLM, where the\\ncascading process inevitably introduces noise and propa-\\ngates errors. More critically, the entire system leverages\\nexisting pre-trained tools for inference only. Due to the lack\\nof overall end-to-end training, the capabilities of content\\nunderstanding and multimodal generation can be very lim-\\nited, especially in interpreting intricate and implicit user\\ninstructions. In a nutshell, there is a compelling need to\\nconstruct an end-to-end MM-LLM of arbitrary modalities.\\nIn pursuit of this goal, we present NExT-GPT, an any-to-\\nany MM-LLM designed to seamlessly handle input and\\noutput in any combination of four modalities: text, image,\\nvideo, and audio. As depicted in Figure 1, NExT-GPT com-\\nprises three tiers. First, we leverage established encoders\\nto encode inputs in various modalities, where these repre-\\nsentations are projected into language-like representations\\ncomprehensible to LLM through a projection layer. Second,\\nwe harness an existing open-sourced LLM as the core to\\nprocess input information for semantic understanding and\\nreasoning. The LLM not only directly generates text tokens\\nbut also produces unique ‘modality signal’ tokens that serve\\nas instructions to dictate the decoding layers on whether\\nand what modal content to output correspondingly. Third,\\nafter projection, the produced multimodal signals with spe-\\ncific instructions are routed to different encoders and finally\\ngenerate content in corresponding modalities.\\nAs NExT-GPT encompasses encoding and generation of\\nvarious modalities, training the system from scratch would\\nentail substantial costs. Instead, we take advantage of the ex-\\nisting pre-trained high-performance encoders and decoders,\\nsuch as CLIP (Radford et al., 2021), ImageBind (Girdhar'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='et al., 2023) and the state-of-the-art latent diffusion models\\n(Rombach et al., 2022; Ruiz et al., 2022; Cerspense, 2023;\\nAn et al., 2023; Liu et al., 2023a; Huang et al., 2023a). By\\nloading the off-the-shelf parameters, we not only avoid cold-\\nstart training but also facilitate the potential growth of more\\nmodalities. For feature alignment across the three tiers,\\nwe only consider fine-tuning locally the input projection\\nand output projection layers, with an encoding-side LLM-\\ncentric alignment and decoding-side instruction-following\\nalignment, where the minimal computational overhead en-\\nsures higher efficiency. Furthermore, to empower our any-\\nto-any MM-LLM with human-level capabilities in com-\\nplex cross-modal generation and reasoning, we introduce\\na modality-switching instruction tuning, to equip the sys-\\ntem with sophisticated cross-modal semantic understanding\\nand content generation. To combat the absence of such\\ncross-modal instruction tuning data in the community, we\\nmanually collect and annotate a MosIT dataset consisting\\nof 5,000 high-quality samples. By employing the LoRA\\ntechnique (Hu et al., 2022), we fine-tune the overall NExT-\\nGPT system on instruction tuning data, updating both input\\nand output projection layers and certain LLM parameters.\\nOverall, this work showcases the promising possibility of\\ndeveloping a more human-like MM-LLM agent capable of\\nmodeling universal modalities. The contributions of this\\n2\\nNExT-GPT: Any-to-Any Multimodal LLM\\nresearch include:\\n• We, for the first time, present an end-to-end general-\\npurpose any-to-any MM-LLM, named NExT-GPT, ca-\\npable of semantic understanding and reasoning and\\ngeneration of free input and output combinations of\\ntext, image, video, and audio.\\n• We introduce lightweight alignment learning tech-\\nniques, the LLM-centric alignment at the encoding\\nside, and the instruction-following alignment at the de-\\ncoding side, efficiently requiring only minimal param-\\neter adjustments (only 1% params) while maintaining\\nhighly effective semantic alignment.\\n• We annotate a high-quality modality-switching in-\\nstruction tuning dataset covering intricate instructions\\nacross various modal combinations of text, image,\\nvideo, and audio, aiding MM-LLM with human-like\\ncross-modal content understanding and reasoning.\\n2. Related Work\\nCross-modal Understanding and Generation\\nOur world\\nis replete with multimodal information, wherein we contin-\\nuously engage in the intricate task of comprehending and\\nproducing cross-modal content. The AI community corre-\\nspondingly emerges varied forms of cross-modal learning\\ntasks (Zeng et al., 2023; Dess`ı et al., 2023; Yang et al., 2021;\\nDing et al., 2021; Liu et al., 2023a; Dorkenwald et al., 2021).\\nMoreover, to generate high-quality content, a multitude of\\nstrong-performing methods have been proposed, such as\\nTransformer (Vaswani et al., 2017; Zhang et al., 2022; Ding\\net al., 2021; Ge et al., 2022), GANs (Liu et al., 2020; Brock\\net al., 2019; Xu et al., 2018; Zhu et al., 2019), VAEs (Vahdat\\n& Kautz, 2020; Razavi et al., 2019), Flow models (Shibata\\net al., 2022; Bashiri et al., 2021) and the current state-of-\\nthe-art diffusion models (Hoogeboom et al., 2021; Qu et al.,\\n2023b; Mou et al., 2023; Feng et al., 2022; Rombach et al.,\\n2022). In particular, the diffusion-based methods have re-\\ncently delivered a remarkable performance in a plethora of\\ncross-modal generation tasks, such as DALL-E (Ramesh\\net al., 2021), Stable Diffusion (Rombach et al., 2022). While\\nall previous efforts of cross-modal learning are limited to\\nthe comprehension of multimodal inputs only, CoDi (Tang\\net al., 2023) lately presents groundbreaking development.\\nLeveraging the power of diffusion models, CoDi possesses\\nthe ability to generate any combination of output modali-'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='ties, including language, image, video, or audio, from any\\ncombination of input modalities in parallel. Regrettably,\\nCoDi still falls short of achieving human-like deep reason-\\ning of input content, because it can only deliver parallel\\ncross-modal feeding&generation without any reasoning and\\ndecision-marking capabilities.\\nMultimodal Large Language Models\\nLLMs have al-\\nready made a profound impact and revolution on the entire\\nAI community and beyond (OpenAI, 2022a;b), where a\\nseries of open-source LLMs have greatly spurred advance-\\nment and made contributions to the community (Chiang\\net al., 2023; Touvron et al., 2023; Zhu et al., 2023; Zhang\\net al., 2023a). Building on top of these LLMs, significant\\nefforts have been made to extend them to deal with mul-\\ntimodal inputs and tasks, leading to the development of\\nMM-LLMs. On the one hand, most researchers build fun-\\ndamental MM-LLMs by aligning the well-trained encoders\\nof various modalities to the textual feature space of LLMs\\nto perceive other modal inputs (Huang et al., 2023c; Zhu\\net al., 2023; Su et al., 2022; Koh et al., 2023). For example,\\nFlamingo (Alayrac et al., 2022) uses a cross-attention layer\\nto connect a frozen image encoder to the LLMs. BLIP-2\\n(Li et al., 2023c) employs a Q-Former to translate the input\\nimage queries to the LLMs. There are also various similar\\npractices for building MM-LLMs that are able to under-\\nstand video (e.g., Video-Chat (Li et al., 2023d) and Video-\\nLLaMA (Zhang et al., 2023c)), audio (e.g., SpeechGPT\\n(Zhang et al., 2023b)), etc. Profoundly, PandaGPT (Su et al.,\\n2023) achieves a comprehensive understanding of six differ-\\nent modalities simultaneously by integrating the multimodal\\nencoder, i.e., ImageBind (Girdhar et al., 2023).\\nNevertheless, these MM-LLMs are all limited to only per-\\nceiving multimodal data, without the ability to generate\\ncontent in arbitrary modalities. To enable LLMs with both\\nmultimodal input and output, some efforts explore employ-\\ning LLMs as decision-makers, and utilizing existing off-the-\\nshelf multimodal encoders and decoders as tools to execute\\nmultimodal input and output, such as Visual-ChatGPT (Wu\\net al., 2023), HuggingGPT (Shen et al., 2023), and Audio-\\nGPT (Huang et al., 2023b). As aforementioned, passing\\nmessages between modules with pure texts (i.e., LLM tex-\\ntual instruction) under the discrete pipeline scheme will\\ninevitably introduce noises. Also, the lack of comprehen-\\nsive tuning across the whole system significantly limits the\\nefficacy of semantics understanding. Our work takes the\\nmutual benefits of both the above two types, i.e., learning\\nan any-to-any MM-LLM in an end-to-end manner.\\n3. Overall Architecture\\nFigure 1 presents the schematic overview of the NExT-GPT\\nframework, consisting of three main stages: encoding, LLM\\nunderstanding and reasoning, and decoding.\\nMultimodal Encoding Stage\\nFirst, we leverage existing\\nwell-established models to encode inputs of various modali-\\nties. There are a set of alternatives of encoders for different\\nmodalities, e.g., CLIP (Radford et al., 2021), HuBERT (Hsu\\net al., 2021). Here we take advantage of the ImageBind\\n(Girdhar et al., 2023), which is a unified high-performance\\nencoder across six modalities. With ImageBind, we are\\nspared from managing many numbers of heterogeneous\\nmodal encoders. Then, via a projection layer, different input\\n3\\nNExT-GPT: Any-to-Any Multimodal LLM\\nTable 1. Summary of NExT-GPT system configuration. Only 1% of parameters need updating during fine-tuning.\\nEncoder\\nInput Projection\\nLLM\\nOutput Projection\\nDiffusion\\nName\\nParam\\nName\\nParam\\nName\\nParam'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Name\\nParam\\nName\\nParam\\nText\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\nImage\\nVicuna\\n7B\\nTransformer\\n31M\\nSD\\n1.3B\\nAudio\\n(LoRA\\n33M\\n)\\nTransformer\\n31M\\nAudioLDM\\n975M\\nVideo\\nImageBind\\n1.2B\\nGrouping\\n28M\\nTransformer\\n32M\\nZeroscope\\n1.8B\\nrepresentations are mapped into language-like representa-\\ntions that are comprehensible to the LLM.\\nLLM Understanding and Reasoning Stage\\nAn LLM\\nis used as the core agent of NExT-GPT. Technically, we\\nemploy the Vicuna (7B-v0) (Chiang et al., 2023), which is\\nthe open-source text-based LLM that is widely used in the\\nexisting MM-LLMs (Su et al., 2023; Zhang et al., 2023c).\\nLLM takes as input the representations from different modal-\\nities and carries out semantic understanding and reasoning\\nover the inputs. It outputs: 1) the textual responses directly,\\nand 2) signal tokens of each modality that serve as instruc-\\ntions to dictate the decoding layers on whether to generate\\nmultimodal contents and what content to produce if yes.\\nMultimodal Generation Stage\\nReceiving the multimodal\\nsignals with specific instructions from LLM (if any), the\\nTransformer-based output projection layers map the sig-\\nnal token representations into the ones that are understand-\\nable to the following multimodal decoders. Technically,\\nwe employ the current off-the-shelf latent conditioned dif-\\nfusion models of different modal generations, i.e., Stable\\nDiffusion (SD-v1.5) for image synthesis (Rombach et al.,\\n2022), Zeroscope (v2-576w) for video synthesis (Cer-\\nspense, 2023), and AudioLDM (l-full) for audio synthe-\\nsis (Liu et al., 2023a). After a projection layer, the signal\\nrepresentations are fed into the conditioned diffusion mod-\\nels for content generation. In Table 1 we summarize the\\noverall system configurations. It is noteworthy that in the\\nentire system, only the input and output projection layers\\nof lower-scale parameters (compared with the overall huge\\ncapacity framework) are required to be updated during the\\nfollowing learning, with all the rest of the encoders and de-\\ncoders frozen. This amounts to, 155M(=28+33+31+31+32)\\n/ [155M + 12.275B(=1.2+7+1.3+1.8+0.975)], or only 1%\\nof parameters need to be updated. This is also one of the\\nkey advantages of our MM-LLM.\\n4. Lightweight Multimodal Alignment\\nLearning\\nTo bridge the gap between the feature space of different\\nmodalities, and ensure fluent semantics understanding of\\ndifferent inputs, it is essential to perform alignment learning\\nfor NExT-GPT. Since we design the loosely-coupled system\\nwith mainly three tiers, we only need to update the two\\nprojection layers at the encoding side and decoding side.\\n4.1. Encoding-side LLM-centric Multimodal Alignment\\nMost\\nexisting\\nMM-LLMs\\nadopt\\nthe\\nTransformer-\\narchitectured multimodal encoders and generate patch-level\\ngrid features (e.g., for image, audio or video).\\nThey\\ntransform the multimodal features to be understandable\\nto the core LLM by projecting them into the text feature\\nspace straightforwardly via linear layers. However, we note\\nthat the patch-based feature units might not best coincide\\nwith the intricate textual token semantics, as intuitively\\nthe language tokens always encapsulate separate concepts.\\nThis may result in suboptimal information perception\\n(Zhong et al., 2022) in MM-LLMs.\\nThus, inspired by\\n(Xu et al., 2022), we design a type of learnable concept\\ntokens to hierarchically aggregate the grid-level features\\ninto semantic concept tokens via a grouping mechanism,\\nand then the conceptual representation is fed into LLM.\\nTo accomplish the alignment, we adopt an ‘X-to-text’ gen-'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='eration task trained on the ‘X-caption’ pair (‘X’ stands for\\nimage, audio, or video) data from existing corpus and bench-\\nmarks, i.e., given the representation of an ‘X’, to prompt the\\nfrozen LLM to generate the corresponding text description.\\nSpecifically, we utilize three types of ‘X-caption’ pair data,\\nincluding 1) ‘Video-caption’ pair dataset: Webvid-2M (Bain\\net al., 2021), a large-scale dataset of short videos with tex-\\ntual description sourced from stock footage sites, 2) ‘Image-\\ncaption’ pair dataset: CC3M (Sharma et al., 2018), contains\\nover 3 million images accompanied by diverse styles of\\nnatural-language descriptions, and 3) ‘Audio-caption’ pair\\ndataset: AudioCaps (Kim et al., 2019), an extensive dataset\\nof approximately 46k audio clips paired with human-written\\ntextual descriptions collected via crowdsourcing. Figure\\n2(a) illustrates the learning process.\\n4.2. Decoding-side Instruction-following Alignment\\nOn the decoding end, we have integrated pre-trained condi-\\ntional diffusion models from external resources. Our main\\npurpose is to align the diffusion models with LLM’s out-\\nput instructions. However, performing a full-scale align-\\nment process between each diffusion model and the LLM\\nwould entail a significant computational burden. Alterna-\\ntively, we explore a more efficient approach, decoding-side\\ninstruction-following alignment, as depicted in Figure 2(b).\\nSpecifically, instead of outputting straightforward textual\\ninstructions, we design three types of special tokens (Koh\\net al., 2023), i.e., ‘[IMGi]’ (i = 0, · · · , 4) as image signal\\n4\\nNExT-GPT: Any-to-Any Multimodal LLM\\nTransformer Layers\\nGrouping Block\\nTransformer Layers\\nGrouping Block\\nText \\nEncoder \\nVideo Diffusion\\nU-Net\\nVideo\\nLinear\\nTransformer\\nEncoder\\nTransformer\\nDecoder\\nVideo Caption\\nCaption-\\nalignment \\nLoss\\nImage \\nEncoder\\nConditional Latent\\nDenoising Loss\\nLinear\\nText \\nEncoder \\nVideo Diffusion\\nU-Net\\nVideo\\nLinear\\nTransformer\\nEncoder\\nTransformer\\nDecoder\\nVideo Caption\\nCaption-\\nalignment \\nLoss\\nImage \\nEncoder\\nConditional Latent\\nDenoising Loss\\nLinear\\nText \\nEncoder \\nAudio Diffusion\\nU-Net\\nAudio\\nLinear\\nTransformer\\nEncoder\\nTransformer\\nDecoder\\nAudio Caption\\nCaption-\\nalignment \\nLoss\\nImage \\nEncoder\\nConditional Latent\\nDenoising Loss\\nLinear\\nText \\nEncoder \\nAudio Diffusion\\nU-Net\\nAudio\\nLinear\\nTransformer\\nEncoder\\nTransformer\\nDecoder\\nAudio Caption\\nCaption-\\nalignment \\nLoss\\nImage \\nEncoder\\nConditional Latent\\nDenoising Loss\\nLinear\\nImage\\nLLM\\nAudio\\nVideo\\nImage \\nCaption\\nAudio \\nCaption\\nVideo \\nCaption\\nAudio \\nEncoder\\nVideo \\nEncoder\\nImage \\nEncoder\\nVid. Patch Rep.\\nConcept Vid. Rep.\\nConcept Aud. Rep.\\nConcept Img. Rep.\\nAud. Patch Rep.\\nImg. Patch Rep.\\nInput Projection\\nConcept Token Rep.\\nPredicted Gold Annotation\\nImage \\nCaption\\nAudio \\nCaption\\nVideo \\nCaption\\nCross Entropy\\nLLM\\nImage Signal Token\\nText Response\\nAudio signal token\\nText Response\\nVideo signal token\\nText Response\\nLLM Output Rep.\\n(b) Decoding-side Instruction-following Alignment\\nText \\nEncoder \\nImage Diffusion\\nU-Net\\nImage\\nLinear\\nTransformer\\nEncoder\\nTransformer\\nDecoder\\nImage Caption\\nCaption-\\nalignment \\nLoss\\nImage \\nEncoder\\nConditional Latent\\nDenoising Loss\\nLinear\\nImage Output Projection\\n(a) Encoding-side LLM-centric Alignment\\nLearnable Queries'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Figure 2. Illustration of the lightweight multimodal alignment learning of encoding and decoding, respectively.\\ntokens; ‘[AUDi]’ (i = 0, · · · , 8) as audio signal tokens;\\nand ‘[VIDi]’ (i = 0, · · · , 24) as video signal tokens; these\\ntokens implicitly carry rich and flexible instructions for the\\ndownstream diffusion model. We want to enable the LLM\\nto learn what content to generate, i.e., textual tokens, and\\nmodality signal tokens. If LLM identifies a certain modality\\ncontent to be produced, a special type of token will be out-\\nput indicating the activation of that modality; otherwise, no\\nspecial token output means deactivation of that modality.\\nWe notice that diffusion models generate contents condi-\\ntioned solely on text-oriented representations, i.e., from\\nthe diffusion textual encoders. However, this text-centered\\nconditioning diverges significantly from the modal signal\\ntokens in our LLM. This leads to a gap that prevents the\\ndiffusion models from accurately interpreting the instruc-\\ntions from LLM. Thus, on the one hand, we consider taking\\nthe LLM’s modal signal token representations (after each\\nTransformer-based project layer) as a conditional input in\\nthe denoising process to guide the diffusion model to gener-\\nate appropriate images, videos, or audio. On the other hand,\\nwe also propose minimizing the distance between projected\\nsignal token representations and the conditional text repre-\\nsentations of the diffusion models to accelerate alignment\\nlearning. Note that all the diffusion backbones (i.e., U-Net)\\nare frozen, which also ensures highly lightweight training.\\nIn the alignment training phase, we take the captions from\\nCC3M, WebVid, and AudioCaps as inputs and concate-\\nnate them with the signal tokens as outputs. The loss func-\\ntion comprises three key components: 1) the negative log-\\nlikelihood of producing signal tokens, and 2) the caption\\nalignment loss: l2-distance between the hidden states of\\nsignal tokens produced by the LLM and the conditional\\ntext representations derived from the text encoder within\\ndiffusion models, and 3) conditional latent denoising loss\\n(Rombach et al., 2022).\\n5. Modality-switching Instruction Tuning\\n5.1. Instruction Tuning\\nDespite aligning both the encoding and decoding ends with\\nLLM, there remains a gap towards the goal of enabling the\\noverall system to faithfully follow and understand users’\\ninstructions and generate the desired multimodal outputs.\\nTo address this, further instruction tuning (IT) (Yin et al.,\\n2023; Su et al., 2023; Liu et al., 2023b) is deemed neces-\\nsary to enhance the capabilities and controllability of LLM.\\n5\\nNExT-GPT: Any-to-Any Multimodal LLM\\nText\\nLLM\\nLoRA\\nImage Input \\nProjection\\nImage \\nEncoder\\nImage \\nDiffusion \\nImage Output \\nProjection\\nAudio Input \\nProjection\\nAudio \\nEncoder\\nVideo \\nEncoder\\nVideo Input \\nProjection\\nAudio \\nDiffusion \\nVideo \\nDiffusion \\nAudio Output \\nProjection\\nVideo Output \\nProjection\\nText +\\nText\\nText +\\nText\\nText +\\nText\\nText\\n+\\nText\\nText\\nText\\nText\\nText\\nText\\nText\\nText\\n<IMGk> ... \\n<AUDn>...\\n<VIDm> ... \\n<IMGk>... \\n…\\nImg. Sig. \\nTok. Rep.\\nAud. Sig. \\nTok. Rep.\\nVid. Sig. \\nTok. Rep.\\n…\\n...\\nAudio Caption\\nVideo Caption\\nInput Instructions\\nCross\\nentropy\\n…\\n…\\n…\\nLLM Output\\nGold Annotation\\nText\\nText\\nText\\nText\\nText\\nText\\nText\\nText\\n<IMGk> ... \\n<AUDn>...\\n<VIDm> ... \\n<IMGk>... \\nGold Annotation\\nImage Caption\\nGeneration \\nLoss\\nFigure 3. Illustration of modality-switching instruction tuning.\\nIT involves additional training of overall MM-LLMs us-\\ning ‘(INPUT, OUTPUT)’ pairs, where ‘INPUT’ represents'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='the user’s instruction, and ‘OUTPUT’ signifies the desired\\nmodel output that conforms to the given instruction. Techni-\\ncally, we leverage LoRA (Hu et al., 2022) to enable a small\\nsubset of parameters within NExT-GPT to be updated con-\\ncurrently with two layers of projection during the IT phase.\\nAs illustrated in Figure 3, when an IT dialogue sample is\\nfed into the system, the LLM reconstructs and generates the\\ntextual content of input (and represents the multimodal con-\\ntent with the multimodal signal tokens). The optimization\\nis imposed based on gold annotations and LLM’s outputs.\\nIn addition to LLM tuning, we also fine-tune the decoding\\nend of NExT-GPT. We align the modal signal tokens’ rep-\\nresentation encoded by the output projection with the gold\\nmultimodal caption representation encoded by the diffu-\\nsion condition encoder. Thereby, the comprehensive tuning\\nprocess brings closer to the goal of faithful and effective\\ninteraction with users.\\n5.2. Instruction Dataset\\nFor the IT of NExT-GPT, we first consider leveraging the\\nwell-established ‘Text’→‘Text+X’ datasets where ‘X’ could\\nbe the image, video, audio, or others, for example, LLaVA-\\n150K (Liu et al., 2023b), and VideoChat (Li et al., 2023d).\\nHowever, these IT datasets are limited to output textual re-\\nsponses from LLMs. In our any-to-any scenario, the target\\nnot only includes the generations of texts, but also the multi-\\nmodal contents, i.e., ‘Text+X’. Thus, we construct the ‘Text’\\n→‘Text+X’ dataset, i.e., text-to-multimodal (namely T2M)\\ndata. Based on the rich volume of ‘X-caption’ pairs from\\nthe existing corpus and benchmarks (Sharma et al., 2018;\\nLin et al., 2014; Bain et al., 2021; Kim et al., 2019), with\\nsome templates, we employ GPT-4 to produce varied textual\\ninstructions to wrap the captions, and result in the dataset.\\nMosIT Dataset\\nCrafting high-quality instructions that\\ncomprehensively cover the desired target behaviors is non-\\ntrivial. We notice that the above IT datasets fail to meet the\\nrequirements for our any-to-any MM-LLM scenario. Firstly,\\nduring a human-machine interaction, users and LLM in-\\nvolve diverse and dynamically changing modalities in their\\ninputs and outputs. Additionally, we allow multi-turn con-\\nversations in the process, and thus the processing and under-\\nstanding of complex user intentions is required. However,\\nthe above two types of datasets lack variable modalities,\\nand also are relatively short in dialogues, failing to mimic\\nreal-world scenarios adequately.\\nTo facilitate the development of any-to-any MM-LLM, we\\npropose a novel Modality-switching Instruction Tuning\\n(MosIT) approach. MosIT not only supports complex\\ncross-modal understanding and reasoning but also enables\\nsophisticated multimodal content generation. In conjunc-\\ntion with MosIT, we manually and meticulously construct\\na high-quality dataset. The MosIT dataset encompasses a\\nwide range of multimodal inputs and outputs, offering the\\nnecessary complexity and variability to facilitate the training\\nof MM-LLMs that can handle diverse user interactions and\\ndeliver the desired responses accurately. Specifically, we de-\\nsign some template dialogue examples between a ‘Human’\\nrole and a ‘Machine’ role, based on which we prompt the\\nGPT-4 to generate more conversations under various scenar-\\nios with more than 100 topics or keywords. The interactions\\nare required to be diversified, e.g., including both straight-\\nforward and implicit requirements by the ‘Human’, and exe-\\ncution of perception, reasoning, suggestion, and planning,\\netc., by the ‘Machine’. And the interactive content should be\\nlogically connected and semantically inherent and complex,\\nwith in-depth reasoning details in each response by the ‘Ma-\\nchine’. Each conversation should include 3-7 turns (i.e., QA'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='pairs), where the ‘Human’-‘Machine’ interactions should\\ninvolve multiple modalities at either the input or output side,\\nand switch the modalities alternately. Whenever multimodal\\ncontents (e.g., image, audio, and video) are present in the\\nconversations, we look for the best-matched contents from\\nthe external resources, including the retrieval systems, e.g.,\\nYoutube, and even AIGC tools, e.g., Stable-XL (Podell et al.,\\n2023), and Midjourney. After human inspections and fil-\\ntering of inappropriate instances, we obtain a total of 5K\\nhigh-quality dialogues. In Table 6 of Appendix §C.4, we\\ncompare the statistics of existing multimodal IT datasets\\nwith our MosIT data in detailed statistics.\\n6\\nNExT-GPT: Any-to-Any Multimodal LLM\\nTable 2. Zero-shot evaluation of image captioning with CIDEr (↑) score on NoCaps (Agrawal et al., 2019), Flickr 30K (Young et al., 2014) and COCO\\n(Karpathy & Fei-Fei, 2017), and image question answering on VQAv2 (Goyal et al., 2017), VizWiz (Gurari et al., 2018) and OKVQA (Marino et al.,\\n2019), and two evaluation-only benchmarks, MMB (Liu et al., 2023c) and SEED (Li et al., 2023a). The best results are marked in bold, and the second\\nones are underlined.\\nModel\\nVersion\\nImage Captioning\\nImage Question Answering\\nComprehensive\\nNoCaps\\nFlickr 30K\\nCOCO\\nVQAv2\\nVizWiz\\nOKVQA\\nMMB\\nSEED\\nInstructBLIP (Dai et al., 2023)\\nVicuna-7B\\n123.1\\n82.4\\n102.2\\n-\\n33.4\\n33.9\\n36.0\\n-\\nLLaVA (Liu et al., 2023b)\\nLLaMA-2-7B-Chat\\n120.7\\n82.7\\n-\\n-\\n-\\n-\\n36.2\\n-\\nmPLUG-Owl (Ye et al., 2023b)\\nLLaMA-7B\\n117.0\\n80.3\\n119.3\\n-\\n39.0\\n-\\n46.6\\n34.0\\nEmu (Sun et al., 2023)\\nLLaMA-7B\\n-\\n-\\n117.7\\n40.0\\n35.4\\n34.7\\n-\\n-\\nDREAMLLM (Dong et al., 2023)\\nVicuna-7B\\n-\\n-\\n115.4\\n56.6\\n45.8\\n44.3\\n49.9\\n-\\nVideo-LLaVA (Lin et al., 2023)\\nVicuna-7B\\n-\\n-\\n-\\n74.7\\n48.1\\n-\\n60.9\\n-\\nNExT-GPT\\nVicuna-7B\\n123.7\\n84.5\\n124.9\\n66.7\\n48.4\\n52.1\\n58.0\\n57.5\\nTable 3. Comparison of video reasoning tasks on MSRVTT (Xu et al., 2016), MSVD-QA and MSRVTT-QA (Xu et al., 2017) and NExTQA (Xiao et al.,\\n2021), and the audio captioning task on AudioCaps (Kim et al., 2019). Scores with ∗means being fine-tuned on the training dataset.\\nModel\\nVersion\\nVideo Captioning\\nVideo Question Answering\\nAudio Captioning\\nMSR-VTT\\nMSVD-QA\\nMSRVTT-QA\\nNExTQA\\nAudioCaps\\nCodi (Tang et al., 2023)\\n-\\n74.4∗\\n-\\n-\\n-\\n78.9∗\\nUIO-2XXL (Lu et al., 2023)\\n6.8B\\n48.8∗\\n41.5\\n52.1\\n-\\n48.9∗\\nVideo-LLaMA (Zhang et al., 2023c)\\nLLaMA-7B\\n-\\n51.6\\n-\\n29.6\\n-\\nVideo-LLaVA (Lin et al., 2023)'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Vicuna-7B\\n-\\n70.7\\n59.2\\n-\\n-\\nEmu (Sun et al., 2023)\\nLLaMA-7B\\n-\\n32.4\\n14.0\\n6.8\\n-\\nNExT-GPT\\nVicuna-7B\\n76.2∗\\n64.5\\n61.4\\n50.7\\n81.3∗\\nTable 4.\\nResults on text-to-image/audio/video generation (MS COCO\\n(Lin et al., 2014), AudioCaps (Kim et al., 2019), and MSRVTT (Xu et al.,\\n2016)). †: zero-shot results.\\nModel\\nImage\\nAudio\\nVideo\\nFID (↓) FAD (↓) CLIPSIM (↑)\\nSD-1.5 (Wang et al., 2022c)\\n11.21\\n-\\n-\\nCodi (Huang et al., 2023a)\\n11.26\\n1.80\\n28.90\\nAudioLDM-L (Liu et al., 2023a)\\n-\\n1.96\\n-\\nGILL-8B† (Koh et al., 2023)\\n12.20\\n-\\n-\\nEmu-13B† (Sun et al., 2023)\\n11.66\\n-\\n-\\nUIO-2XXL (Lu et al., 2023)\\n13.39\\n2.64\\n-\\nNExT-GPT\\n10.07\\n1.68\\n31.97\\nNExT-GPT†\\n11.18\\n1.74\\n30.96\\n6. Experiments\\nIn the experiments, we aim to quantify the performance\\nof NExT-GPT on a range of downstream tasks requiring\\nperceiving and generating any modalities. More settings\\nand implementation details can be found in Appendix §C.\\nAlso due to the space limitation, we present a good number\\nof more experimental results and analyses in Appendix §D.\\n6.1. Main Results\\nMultimodal Perception\\nFirstly, we evaluate the semantic\\nunderstanding capability of the NExT-GPT w.r.t. image,\\nvideo, or audio, across multiple benchmarks. The results are\\nshown in Table 2, and 3. Notably, NExT-GPT showcases\\nexceptional performance in image comprehension, demon-\\nstrating significant improvements over baseline levels in\\ntasks such as image captioning and image question answer-\\ning. Moreover, when evaluated on evaluation-only bench-\\nmark datasets like MMBench (MMB) and SEED-Bench\\n(SEED), NExT-GPT consistently achieves comparable per-\\nformance. Additionally, the model excels in video and audio\\ncomprehension. In comparison with Codi, NExT-GPT at-\\ntains enhanced results attributed to its capability for direct\\ntext generation from LLM, leveraging the inherent expertise\\nof the LLM.\\nMultimodal Generation\\nWe then examine the synthesis\\nquality of the image, video, or audio conditioned on text.\\nTable 4 presents the comparisons between ours and some\\nstate-of-the-art systems. Overall, NExT-GPT exhibits su-\\nperior performance in generating images, audio, and video\\nconditioned on text. Compared to LLM-centric models, i.e.,\\nGILL, Emu, and UIO-2XXL, ours stands out by support-\\ning a more diverse range of modalities. Moreover, in the\\ngeneration of individual modalities, NExT-GPT maintains\\noptimal performance, even in zero-shot scenarios. Notably,\\nin comparison with non-LLM-centered models, ours still\\ndemonstrates a clear improvement in generation quality.\\n6.2. In-depth Analysis\\nThe Impact of Signal Token Numbers\\nIn Figure 6, we\\nshow the results of NExT-GPT utilizing varying numbers of\\nproposed modality-specific signal tokens. The experimental\\nresults reveal that the number of tokens required varies for\\neach modality. Notably, videos, due to their more intricate\\ncontent generation, demand the highest number of signal\\n7\\nNExT-GPT: Any-to-Any Multimodal LLM\\nWhat is the unusual part of \\nthis video?\\nThe unusual part of this video is that a dog is \\nriding a skateboard. It is not common to see a \\ndog on a skateboard, as dogs are typically not \\nable to ride skateboards or bicycles. This scene'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content=\"is unique and interesting because it showcases \\na dog's ability to adapt and participate in \\nhuman-made \\nactivities, \\ndemonstrating \\nits \\nintelligence and adaptability.\\nThat is pretty cool. How about adding some \\nmusic to make this scene look more funny?\\nCertainly! Here's a synthesized music track \\nthat \\ncould \\nenhance \\nthe \\nhumor \\nand \\nlightheartedness of the scene:\\nDo you have any other similar unusual scenes? \\nShow them.\\nI have a set of such \\nkinds of scenes:\\nI am in a bad mood, so bad so sad.\\nI am sorry to hear about that. Sometimes, it is \\nhard to shake off a bad mood. Have you tried \\nlistening to some lifting music or watching a \\ncomedy to cheer you up?\\nCan you show something to cheer me up?\\nSure, here's a video of \\na cute puppy playing \\nin the park. It always \\nmakes me feel better. \\nI need to prepare my presentation for my \\nhistory class, can you give some inspiration?\\n0f course! History is a fascinating subject. One \\napproach you could take is to create a timeline \\npresentation. You can use images, videos, and \\ntext to show the progression of events and \\nhighlight key moments in history.  ...\\nCan you show me some examples?\\nCertainly! Here are a few examples of timeline \\npresentations and historical figures :1.2.3.\\n(A)\\n(B)\\n(C)\\nFigure 4. Qualitative examples showcasing the interpretative and generative capabilities of NExT-GPT across diverse modalities or their combinations.\\nInstruction Following\\nRationality\\nQuality\\n65\\n72\\n79\\n86\\nEvaluation Scale\\nHuggingGPT\\nVisual-ChatGPT\\nNExT-GPT-caption\\nNExT-GPT\\nFigure 5. Human Evaluation (1-100 scale, results are on average) of\\nNExT-GPT in comparison with pipeline baselines.\\ntokens. The other two modalities, images and audio, achieve\\nsatisfactory generation with merely 4 and 8 signal tokens,\\nrespectively. However, the choice of signal token numbers\\nis contingent on factors such as training data size and the\\nselection of the diffusion model. For example, with more\\nextensive data and a robust diffusion model, increasing the\\nnumber of signal tokens might lead to improved results.\\nThe Impact of Grouping Mechanism\\nTo further illus-\\ntrate the effectiveness of employing the grouping mech-\\nanism to align visual features with LLM, we conducted\\nexperiments with different projection architecture designs.\\nThese designs include ‘w Linea Layer’ which removes the\\ngrouping module and directly maps the output of Imagebind\\nto the language embedding space through a single linear\\nlayer, and ‘w Q-former + Linea Layer’ which integrates\\nQ-former instead of the grouping mechanism. All variants\\nundergo training following the same procedure as the origi-\\nnal design. The results of two image QA datasets, two video\\nQA datasets, and an audio captioning dataset are presented\\nin Table 4. The experimental findings indicate a significant\\ndecrease in the model’s perceptual capabilities across three\\nmodalities when using a simple linear approach. In addition,\\nthe integration of Q-former yields a modest improvement\\nin perceptual capabilities. This enhancement might be at-\\ntributed to the Q-former’s ability to perform slight visual\\nfeature grouping, aligning effectively with complex tex-\\ntual token semantics, thus elevating perceptual capabilities.\\nHowever, our grouping mechanism of NExT-GPT shows\\nthe optimal performance.\\nEvaluation on Pipeline vs End-to-End MM-LLMs\\nTo\\nevaluate if the system really or how well it understands the\\ninput and generates output content (response text + image),\\nwe perform the human evaluation. For constructing the test-\\ning data, we first leverage GPT-4 to synthesize 100 complex\\ninstructions (e.g., involving intricate and semantically-rich\\nscenes) that require implicit reasoning ability to generate im-\\nage content. Then, the synthesized instructions are fed into\\nthe models to generate the response text + image content.\\n8\\nNExT-GPT: Any-to-Any Multimodal LLM\\n1\\n2\\n4\\n8\\n9\\n10\\n11\\n12\\n(a) Text-to-image Generation\"),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='FID(↓)\\n4\\n8\\n16\\n24\\n32\\n13\\n14\\n15\\n16\\n(b) Text-to-video Generation\\nFID(↓)\\n2\\n4\\n8\\n16\\n1\\n2\\n3\\n4\\n(c) Text-to-audio Generation\\nFAD(↓)\\nFigure 1: Interaction figures\\nFigure 6. The generation quality under different numbers of modality signal tokens.\\nTable 5.\\nThe perception performance of NExT-GPT by varying input projection mechanisms.\\nModel\\nImage Question Answering\\nVideo Question Answering\\nAudio Captioning\\nVQAv2\\nVizWiz\\nMSVD-QA\\nMSRVTT-QA\\nAudioCaps\\nNExT-GPT\\n66.7\\n48.4\\n64.5\\n61.4\\n81.3\\nw Linear Layer\\n63.8\\n45.4\\n60.8\\n57.1\\n77.4\\nw Q-former + Linear Layer\\n65.1\\n46.9\\n63.4\\n58.1\\n79.7\\nSubsequently, five unbiased volunteers evaluate the gener-\\nated results under three aspects, 1) Instruction following,\\nidentifying, among the four models, which of the generated\\ntext+image accurately responded to the input instructions,\\n2) Rationality, determining which of the generated images\\nadhered to the input instructions, 3) Quality, evaluating\\nwhich of the generated images exhibited the highest qual-\\nity. Figure 5 illustrates superior performance in following\\ncomplex instructions and generating high-quality images,\\ncompared to two existing systems and NExT-GPT-caption,\\nwhich directly generates textual captions for downstream\\ndiffusion models.\\nQualitative Analysis\\nTo directly demonstrate the effec-\\ntiveness and potential of NExT-GPT in developing human-\\nlike conversational agents, we further offer qualitative ex-\\namples that vividly illustrate the system’s capacity to com-\\nprehend and reason contents across various modalities in\\nany combination, as shown in Figure 4. From example (A),\\nwe can note that NExT-GPT can understand the unusual\\npart of the input video, and synthesize a light-heartedness\\naudio and similar unusual scenes, i.e., a cat riding a skate-\\nboard. In addition, beyond responding to explicit queries\\nprompting model synthesis in specific modalities, NExT-\\nGPT demonstrates proficiency in inferring implicit user\\nintentions. In example (B), when the user conveys a neg-\\native mood, NExT-GPT responds empathetically and au-\\ntonomously and decides to present a cheerful puppy video\\nto uplift the user’s spirits. Similarly, when preparing a pre-\\nsentation for a history class, NExT-GPT exhibits flexibility\\nin generating pertinent tips and visualizations. Kindly refer\\nto Appendix §D.4 for more demonstrations with implicit\\nand explicit instructions.\\n7. Conclusion\\nIn this work, we presented an end-to-end general-purpose\\nany-to-any multimodal Large Language Model (MM-LLM).\\nBy connecting an LLM with multimodal adaptors and dif-\\nferent diffusion decoders, NExT-GPT is capable of perceiv-\\ning inputs and generating outputs in any combination of\\ntext, image, video, and audio. Harnessing the existing well-\\ntrained highly-performing encoders and decoders, training\\nNExT-GPT only entails a few number of parameters (1%)\\nof certain projection layers, which not only benefits low\\ncosts but also facilitates convenient expansion of more po-\\ntential modalities in the future. To enable our NExT-GPT\\nwith complex cross-modal semantic understanding and con-\\ntent generation, we further introduced a modality-switching\\ninstruction tuning (MosIT), and manually curated a high-\\nquality dataset for MosIT. Overall, our research showcases\\nthe potential of any-to-any MM-LLMs in bridging the gap\\nbetween various modalities and paving the way for more\\nhuman-like AI systems in the future.\\nAcknowledgements\\nThis work is supported by CCF-Baidu Open Fund and NExT\\nResearch Center.\\nImpact Statement\\nThis paper aims to develop a human-level AI agent, an end-\\nto-end general-purpose any-to-any MM-LLM. The NExT-\\nGPT, constrained by the quantity of fine-tuning data and the'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='quality of base models, may produce low-quality or halluci-\\nnated content that could be harmful. Users are cautioned to\\ninterpret results carefully and adhere to licensing rules, with\\ncommercial use prohibited. We prioritize data privacy by\\nfollowing social media platform terms and obtaining user\\nconsent when necessary, ensuring all personal information\\nis anonymized or obfuscated. Additionally, we are vigilant\\nin minimizing bias in dataset collection, striving for a repre-\\nsentative and fair dataset that does not favor or disfavor any\\nparticular group or perspective.\\n9\\nNExT-GPT: Any-to-Any Multimodal LLM\\nReferences\\nAgrawal, H., Anderson, P., Desai, K., Wang, Y., Chen, X.,\\nJain, R., Johnson, M., Batra, D., Parikh, D., and Lee, S.\\nnocaps: novel object captioning at scale. In Proceedings\\nof the ICCV, pp. 8947–8956, 2019.\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Has-\\nson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong,\\nZ., Samangooei, S., Monteiro, M., Menick, J. L.,\\nBorgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh,\\nS., Binkowski, M., Barreira, R., Vinyals, O., Zisserman,\\nA., and Simonyan, K. Flamingo: a visual language model\\nfor few-shot learning. In Proceedings of the NeurIPS,\\n2022.\\nAn, J., Zhang, S., Yang, H., Gupta, S., Huang, J., Luo,\\nJ., and Yin, X. Latent-shift: Latent diffusion with tem-\\nporal shift for efficient text-to-video generation. CoRR,\\nabs/2304.08477, 2023.\\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\\nGould, S., and Zhang, L. Bottom-up and top-down atten-\\ntion for image captioning and visual question answering.\\nIn Proceedings of the CVPR, pp. 6077–6086, 2018.\\nAvrahami, O., Fried, O., and Lischinski, D. Blended la-\\ntent diffusion. ACM Trans. Graph., 42(4):149:1–149:11,\\n2023.\\nBain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen\\nin time: A joint video and image encoder for end-to-end\\nretrieval. In Proceedings of the ICCV, pp. 1708–1718,\\n2021.\\nBashiri, M., Walker, E. Y., Lurz, K., Jagadish, A., Muham-\\nmad, T., Ding, Z., Ding, Z., Tolias, A. S., and Sinz, F. H.\\nA flow-based latent state generative model of neural pop-\\nulation responses to natural images. In Proceedings of\\nthe NeurIPS, pp. 15801–15815, 2021.\\nBrock, A., Donahue, J., and Simonyan, K. Large scale\\nGAN training for high fidelity natural image synthesis.\\nIn Proceedings of the ICLR, 2019.\\nCerspense. Zeroscope: Diffusion-based text-to-video syn-\\nthesis. 2023. URL https://huggingface.co/\\ncerspense.\\nCeylan, D., Huang, C. P., and Mitra, N. J. Pix2video: Video\\nediting using image diffusion. CoRR, abs/2303.12688,\\n2023.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\\nStoica, I., and Xing, E. P. Vicuna: An open-source chatbot\\nimpressing gpt-4 with 902023.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S.,\\nWebson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,\\nChowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao,\\nV. Y., Huang, Y., Dai, A. M., Yu, H., Petrov, S., Chi, E. H.,\\nDean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and\\nWei, J. Scaling instruction-finetuned language models,\\n2022.\\nCouairon, G., Verbeek, J., Schwenk, H., and Cord, M.\\nDiffedit: Diffusion-based semantic image editing with\\nmask guidance. In Proceedings of the ICLR, 2023.\\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang, W.,\\nLi, B., Fung, P., and Hoi, S. C. H. Instructblip: Towards\\ngeneral-purpose vision-language models with instruction\\ntuning. CoRR, abs/2305.06500, 2023.\\nDess`ı, R., Bevilacqua, M., Gualdoni, E., Rakotonirina, N. C.,\\nFranzon, F., and Baroni, M. Cross-domain image caption-\\ning with discriminative finetuning. In Proceedings of the\\nCVPR, pp. 6935–6944, 2023.\\nDing, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D.,\\nLin, J., Zou, X., Shao, Z., Yang, H., and Tang, J. Cogview:\\nMastering text-to-image generation via transformers. In\\nProceedings of the NeurIPS, pp. 19822–19835, 2021.\\nDong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao,\\nL., Sun, J., Zhou, H., Wei, H., Kong, X., Zhang, X.,\\nMa, K., and Yi, L. Dreamllm: Synergistic multimodal\\ncomprehension and creation. CoRR, abs/2309.11499,\\n2023.\\nDorkenwald, M., Milbich, T., Blattmann, A., Rombach, R.,\\nDerpanis, K. G., and Ommer, B. Stochastic image-to-\\nvideo synthesis using cinns. In Proceedings of the CVPR,\\npp. 3742–3753, 2021.\\nFan, W., Chen, Y., Chen, D., Cheng, Y., Yuan, L., and Wang,\\nY. F. Frido: Feature pyramid diffusion for complex scene\\nimage synthesis. CoRR, abs/2208.13753, 2022.\\nFeng, W., He, X., Fu, T., Jampani, V., Akula, A. R.,\\nNarayana, P., Basu, S., Wang, X. E., and Wang, W. Y.\\nTraining-free structured diffusion guidance for composi-\\ntional text-to-image synthesis. CoRR, abs/2212.05032,\\n2022.\\nFeng, W., Zhu, W., Fu, T., Jampani, V., Akula, A. R., He,\\nX., Basu, S., Wang, X. E., and Wang, W. Y. Layoutgpt:\\nCompositional visual planning and generation with large\\nlanguage models. CoRR, abs/2305.15393, 2023.\\nGal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano,\\nA. H., Chechik, G., and Cohen-Or, D. An image is worth\\none word: Personalizing text-to-image generation using\\ntextual inversion. CoRR, abs/2208.01618, 2022.\\n10\\nNExT-GPT: Any-to-Any Multimodal LLM\\nGe, S., Hayes, T., Yang, H., Yin, X., Pang, G., Jacobs, D.,\\nHuang, J., and Parikh, D. Long video generation with'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='time-agnostic VQGAN and time-sensitive transformer.\\nIn Proceedings of the ECCV, pp. 102–118, 2022.\\nGe, Y., Ge, Y., Zeng, Z., Wang, X., and Shan, Y. Plant-\\ning a SEED of vision in large language model. CoRR,\\nabs/2307.08041, 2023.\\nGemmeke, J. F., Ellis, D. P. W., Freedman, D., Jansen, A.,\\nLawrence, W., Moore, R. C., Plakal, M., and Ritter, M.\\nAudio set: An ontology and human-labeled dataset for\\naudio events. In Proceedings of the ICASSP, pp. 776–780,\\n2017.\\nGirdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K. V.,\\nJoulin, A., and Misra, I. Imagebind: One embedding\\nspace to bind them all. CoRR, abs/2305.05665, 2023.\\nGontier, F., Serizel, R., and Cerisara, C. Automated audio\\ncaptioning by fine-tuning BART with audioset tags. In\\nProceedings of the DCASE, pp. 170–174, 2021.\\nGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and\\nParikh, D. Making the V in VQA matter: Elevating\\nthe role of image understanding in visual question an-\\nswering. In Proceedings of the CVPR, pp. 6325–6334,\\n2017.\\nGu, X., Chen, G., Wang, Y., Zhang, L., Luo, T., and Wen,\\nL. Text with knowledge graph augmented transformer\\nfor video captioning. In Proceedings of the CVPR, pp.\\n18941–18951, 2023.\\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\\nAnswering visual questions from blind people. In Pro-\\nceedings of the CVPR, pp. 3608–3617, 2018.\\nHertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch,\\nY., and Cohen-Or, D. Prompt-to-prompt image editing\\nwith cross-attention control. In Proceedings of the ICLR,\\n2023.\\nHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J.\\nCogvideo: Large-scale pretraining for text-to-video gen-\\neration via transformers. CoRR, abs/2205.15868, 2022.\\nHoogeboom, E., Nielsen, D., Jaini, P., Forr´e, P., and Welling,\\nM. Argmax flows and multinomial diffusion: Towards\\nnon-autoregressive language models. CoRR, 2021.\\nHsu, W., Bolte, B., Tsai, Y. H., Lakhotia, K., Salakhutdinov,\\nR., and Mohamed, A. Hubert: Self-supervised speech\\nrepresentation learning by masked prediction of hidden\\nunits. IEEE ACM Trans. Audio Speech Lang. Process.,\\n29:3451–3460, 2021.\\nHu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,\\nS., Wang, L., and Chen, W. Lora: Low-rank adaptation\\nof large language models. In Proceedings of the ICLR,\\n2022.\\nHuang, R., Huang, J., Yang, D., Ren, Y., Liu, L., Li, M.,\\nYe, Z., Liu, J., Yin, X., and Zhao, Z. Make-an-audio:\\nText-to-audio generation with prompt-enhanced diffusion\\nmodels. In Proceedings of the ICML, pp. 13916–13932,\\n2023a.\\nHuang, R., Li, M., Yang, D., Shi, J., Chang, X., Ye, Z.,\\nWu, Y., Hong, Z., Huang, J., Liu, J., Ren, Y., Zhao, Z.,\\nand Watanabe, S. Audiogpt: Understanding and gen-'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='erating speech, music, sound, and talking head. CoRR,\\nabs/2304.12995, 2023b.\\nHuang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma,\\nS., Lv, T., Cui, L., Mohammed, O. K., Patra, B., Liu,\\nQ., Aggarwal, K., Chi, Z., Bjorck, J., Chaudhary, V.,\\nSom, S., Song, X., and Wei, F. Language is not all you\\nneed: Aligning perception with language models. CoRR,\\nabs/2302.14045, 2023c.\\nHuang, W., Tu, S., and Xu, L. Pfb-diff: Progressive feature\\nblending diffusion for text-driven image editing. CoRR,\\nabs/2306.16894, 2023d.\\nKarpathy, A. and Fei-Fei, L. Deep visual-semantic align-\\nments for generating image descriptions. IEEE Trans.\\nPattern Anal. Mach. Intell., 39(4):664–676, 2017.\\nKarras, J., Holynski, A., Wang, T., and Kemelmacher-\\nShlizerman, I. Dreampose: Fashion image-to-video syn-\\nthesis via stable diffusion. CoRR, abs/2304.06025, 2023.\\nKim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Gen-\\nerating captions for audios in the wild. In Proceedings of\\nthe NAACL, pp. 119–132, 2019.\\nKim, E., Kim, J., Oh, Y., Kim, K., Park, M., Sim, J., Lee,\\nJ., and Lee, K. Improving audio-language learning with\\nmixgen and multi-level test-time augmentation. CoRR,\\nabs/2210.17143, 2022.\\nKingma, D. P. and Ba, J. Adam: A method for stochas-\\ntic optimization. In Bengio, Y. and LeCun, Y. (eds.),\\nProceedings of the ICLR, 2015.\\nKoh, J. Y., Fried, D., and Salakhutdinov, R.\\nGenerat-\\ning images with multimodal language models. CoRR,\\nabs/2305.17216, 2023.\\nLi, B., Wang, R., Wang, G., Ge, Y., Ge, Y., and Shan, Y.\\nSeed-bench: Benchmarking multimodal llms with gener-\\native comprehension. CoRR, abs/2307.16125, 2023a.\\n11\\nNExT-GPT: Any-to-Any Multimodal LLM\\nLi, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J.,\\nLi, C., and Liu, Z. MIMIC-IT: multi-modal in-context\\ninstruction tuning. CoRR, abs/2306.05425, 2023b.\\nLi, J., Li, D., Savarese, S., and Hoi, S. C. H. BLIP-2: boot-\\nstrapping language-image pre-training with frozen image\\nencoders and large language models. In Proceedings of\\nthe ICML, pp. 19730–19742, 2023c.\\nLi, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang,\\nY., Wang, L., and Qiao, Y. Videochat: Chat-centric video\\nunderstanding. CoRR, abs/2305.06355, 2023d.\\nLi, L., Yin, Y., Li, S., Chen, L., Wang, P., Ren, S., Li, M.,\\nYang, Y., Xu, J., Sun, X., Kong, L., and Liu, Q. M3it:\\nA large-scale dataset towards multi-modal multilingual\\ninstruction tuning. CoRR, abs/2306.04387, 2023e.\\nLi, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang,\\nL., Hu, H., Dong, L., Wei, F., Choi, Y., and Gao, J. Oscar:\\nObject-semantics aligned pre-training for vision-language'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='tasks. In Proceedings of the ECCV, pp. 121–137, 2020.\\nLi, Y., Wang, X., Xiao, J., Ji, W., and Chua, T. Invariant\\ngrounding for video question answering. In Proceedings\\nof the CVPR, pp. 2918–2927, 2022.\\nLi, Y., Zhang, C., Yu, G., Wang, Z., Fu, B., Lin, G., Shen,\\nC., Chen, L., and Wei, Y. Stablellava: Enhanced visual\\ninstruction tuning with synthesized image-dialogue data.\\nCoRR, abs/2308.10253, 2023f.\\nLin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., and Yuan,\\nL. Video-llava: Learning united visual representation\\nby alignment before projection. CoRR, abs/2311.10122,\\n2023.\\nLin, K., Li, L., Lin, C., Ahmed, F., Gan, Z., Liu, Z., Lu, Y.,\\nand Wang, L. Swinbert: End-to-end transformers with\\nsparse attention for video captioning. In Proceedings of\\nthe CVPR, pp. 17928–17937, 2022.\\nLin, T., Maire, M., Belongie, S. J., Hays, J., Perona, P.,\\nRamanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft\\nCOCO: common objects in context. In Fleet, D. J., Pajdla,\\nT., Schiele, B., and Tuytelaars, T. (eds.), Proceedings of\\nthe ECCV, pp. 740–755, 2014.\\nLiu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D. P.,\\nWang, W., and Plumbley, M. D. Audioldm: Text-to-audio\\ngeneration with latent diffusion models. In Proceedings\\nof the ICML, pp. 21450–21474, 2023a.\\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\\ntuning. CoRR, abs/2304.08485, 2023b.\\nLiu, S., Wang, T., Bau, D., Zhu, J., and Torralba, A. Diverse\\nimage generation via self-conditioned gans. In Proceed-\\nings of the CVPR, pp. 14274–14283, 2020.\\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,\\nYuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin,\\nD. Mmbench: Is your multi-modal model an all-around\\nplayer? CoRR, abs/2307.06281, 2023c.\\nLu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten,\\nR., Hoiem, D., and Kembhavi, A. Unified-io 2: Scaling\\nautoregressive multimodal models with vision, language,\\naudio, and action. CoRR, abs/2312.17172, 2023.\\nMaaz, M., Rasheed, H. A., Khan, S. H., and Khan, F. S.\\nVideo-chatgpt: Towards detailed video understanding via\\nlarge vision and language models. CoRR, abs/2306.05424,\\n2023.\\nMarino, K., Rastegari, M., Farhadi, A., and Mottaghi, R.\\nOK-VQA: A visual question answering benchmark re-\\nquiring external knowledge. In Proceedings of the CVPR,\\npp. 3195–3204, 2019.\\nMeng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J., and\\nErmon, S. Sdedit: Guided image synthesis and editing\\nwith stochastic differential equations. In Proceedings of\\nthe ICLR, 2022.\\nMilewski, V. S. J., Moens, M., and Calixto, I. Are scene\\ngraphs good enough to improve image captioning? In\\nProceedings of the AACL, pp. 504–515, 2020.'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., and\\nQie, X. T2i-adapter: Learning adapters to dig out more\\ncontrollable ability for text-to-image diffusion models.\\narXiv preprint arXiv:2302.08453, 2023.\\nNichol, A. Q., Dhariwal, P., Ramesh, A., Shyam, P.,\\nMishkin, P., McGrew, B., Sutskever, I., and Chen, M.\\nGLIDE: towards photorealistic image generation and edit-\\ning with text-guided diffusion models. In Proceedings of\\nthe ICML, pp. 16784–16804, 2022.\\nOpenAI. Introducing chatgpt. 2022a.\\nOpenAI. Gpt-4 technical report. 2022b.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,\\nC. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,\\nRay, A., Schulman, J., Hilton, J., Kelton, F., Miller, L.,\\nSimens, M., Askell, A., Welinder, P., Christiano, P. F.,\\nLeike, J., and Lowe, R. Training language models to\\nfollow instructions with human feedback. In Proceedings\\nof the NeurIPS, 2022.\\nPerazzi, F., Pont-Tuset, J., McWilliams, B., Gool, L. V.,\\nGross, M. H., and Sorkine-Hornung, A. A benchmark\\ndataset and evaluation methodology for video object seg-\\nmentation. In Proceedings of the CVPR, pp. 724–732,\\n2016.\\n12\\nNExT-GPT: Any-to-Any Multimodal LLM\\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\\nT., M¨uller, J., Penna, J., and Rombach, R. SDXL: im-\\nproving latent diffusion models for high-resolution image\\nsynthesis. CoRR, abs/2307.01952, 2023.\\nQu, L., Wu, S., Fei, H., Nie, L., and Chua, T. Layoutllm-t2i:\\nEliciting layout guidance from LLM for text-to-image\\ngeneration. In Proceedings of the ACM MM, pp. 643–654,\\n2023a.\\nQu, L., Wu, S., Fei, H., Nie, L., and Chua, T. Layoutllm-t2i:\\nEliciting layout guidance from LLM for text-to-image\\ngeneration. CoRR, abs/2308.05095, 2023b.\\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark,\\nJ., Krueger, G., and Sutskever, I. Learning transferable\\nvisual models from natural language supervision.\\nIn\\nProceedings of the ICML, pp. 8748–8763, 2021.\\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad-\\nford, A., Chen, M., and Sutskever, I. Zero-shot text-\\nto-image generation. In Proceedings of the ICML, pp.\\n8821–8831, 2021.\\nRazavi, A., van den Oord, A., and Vinyals, O. Generating\\ndiverse high-fidelity images with VQ-VAE-2. In Proceed-\\nings of the NeurIPS, pp. 14837–14847, 2019.\\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\\nOmmer, B. High-resolution image synthesis with latent\\ndiffusion models. In Proceedings of the CVPR, pp. 10674–\\n10685, 2022.\\nRuiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M.,\\nand Aberman, K. Dreambooth: Fine tuning text-to-image\\ndiffusion models for subject-driven generation. CoRR,'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='abs/2208.12242, 2022.\\nSharma, P., Ding, N., Goodman, S., and Soricut, R. Con-\\nceptual captions: A cleaned, hypernymed, image alt-text\\ndataset for automatic image captioning. In Proceedings\\nof the ACL, pp. 2556–2565, 2018.\\nShen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang,\\nY. Hugginggpt: Solving AI tasks with chatgpt and its\\nfriends in huggingface. CoRR, abs/2303.17580, 2023.\\nShibata, H., Hanaoka, S., Cao, Y., Yoshikawa, M., Take-\\nnaga, T., Nomura, Y., Hayashi, N., and Abe, O. Local\\ndifferential privacy image generation using flow-based\\ndeep generative models. CoRR, abs/2212.10688, 2022.\\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang,\\nS., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh,\\nD., Gupta, S., and Taigman, Y. Make-a-video: Text-\\nto-video generation without text-video data.\\nCoRR,\\nabs/2209.14792, 2022.\\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D. M., Lowe,\\nR., Voss, C., Radford, A., Amodei, D., and Christiano,\\nP. F. Learning to summarize with human feedback. In\\nProceedings of the NeurIPS, 2020.\\nSu, Y., Lan, T., Liu, Y., Liu, F., Yogatama, D., Wang,\\nY., Kong, L., and Collier, N.\\nLanguage models can\\nsee: Plugging visual controls in text generation. CoRR,\\nabs/2205.02655, 2022.\\nSu, Y., Lan, T., Li, H., Xu, J., Wang, Y., and Cai, D.\\nPandagpt: One model to instruction-follow them all.\\nCoRR, abs/2305.16355, 2023.\\nSun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y.,\\nGao, H., Liu, J., Huang, T., and Wang, X. Generative\\npretraining in multimodality. CoRR, abs/2307.05222,\\n2023.\\nTang, Z., Yang, Z., Zhu, C., Zeng, M., and Bansal, M.\\nAny-to-any generation via composable diffusion. CoRR,\\nabs/2305.11846, 2023.\\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,\\nGuestrin, C., Liang, P., and Hashimoto, T. B.\\nStan-\\nford alpaca:\\nAn instruction-following llama model.\\n2023. URL https://github.com/tatsu-lab/\\nstanford_alpaca.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels. CoRR, abs/2302.13971, 2023.\\nVahdat, A. and Kautz, J. NVAE: A deep hierarchical varia-\\ntional autoencoder. In Proceedings of the NeurIPS, 2020.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention\\nis all you need. In Proceedings of the NeurIPS, pp. 5998–\\n6008, 2017.\\nVeaux, C., Yamagishi, J., MacDonald, K., et al.\\nCstr\\nvctk corpus: English multi-speaker corpus for cstr voice\\ncloning toolkit. CSTR, 6:15, 2017.'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Voynov, A., Chu, Q., Cohen-Or, D., and Aberman, K. P+:\\nextended textual conditioning in text-to-image generation.\\nCoRR, abs/2303.09522, 2023.\\nWang, J., Yang, Z., Hu, X., Li, L., Lin, K., Gan, Z., Liu, Z.,\\nLiu, C., and Wang, L. GIT: A generative image-to-text\\ntransformer for vision and language. Trans. Mach. Learn.\\nRes., 2022, 2022a.\\n13\\nNExT-GPT: Any-to-Any Multimodal LLM\\nWang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J.,\\nZhou, C., Zhou, J., and Yang, H. OFA: unifying architec-\\ntures, tasks, and modalities through a simple sequence-\\nto-sequence learning framework. In Proceedings of the\\nICML, volume 162, 2022b.\\nWang, T., Yi, J., Fu, R., Tao, J., and Wen, Z. Campnet:\\nContext-aware mask prediction for end-to-end text-based\\nspeech editing. IEEE ACM Trans. Audio Speech Lang.\\nProcess., 30:2241–2254, 2022c.\\nWu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N.\\nVisual chatgpt: Talking, drawing and editing with visual\\nfoundation models. CoRR, abs/2303.04671, 2023.\\nWu, J. Z., Ge, Y., Wang, X., Lei, W., Gu, Y., Hsu, W.,\\nShan, Y., Qie, X., and Shou, M. Z. Tune-a-video: One-\\nshot tuning of image diffusion models for text-to-video\\ngeneration. CoRR, abs/2212.11565, 2022.\\nXiao, J., Shang, X., Yao, A., and Chua, T. Next-qa: Next\\nphase of question-answering to explaining temporal ac-\\ntions. In Proceedings of the CVPR, pp. 9777–9786, 2021.\\nXiao, J., Yao, A., Liu, Z., Li, Y., Ji, W., and Chua, T. Video\\nas conditional graph hierarchy for multi-granular question\\nanswering. In Proceedings of the AAAI, pp. 2804–2812,\\n2022.\\nXu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X.,\\nand Zhuang, Y. Video question answering via gradu-\\nally refined attention over appearance and motion. In\\nProceedings of the ACM MM, pp. 1645–1653, 2017.\\nXu, H., Ye, Q., Yan, M., Shi, Y., Ye, J., Xu, Y., Li, C., Bi,\\nB., Qian, Q., Wang, W., Xu, G., Zhang, J., Huang, S.,\\nHuang, F., and Zhou, J. mplug-2: A modularized multi-\\nmodal foundation model across text, image and video. In\\nProceedings of the ICML, pp. 38728–38748, 2023.\\nXu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: A large\\nvideo description dataset for bridging video and language.\\nIn Proceedings of the CVPR, pp. 5288–5296, 2016.\\nXu, J., Mello, S. D., Liu, S., Byeon, W., Breuel, T. M.,\\nKautz, J., and Wang, X. Groupvit: Semantic segmenta-\\ntion emerges from text supervision. In Proceedings of the\\nCVPR, pp. 18113–18123, 2022.\\nXu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang,\\nX., and He, X. Attngan: Fine-grained text to image gen-\\neration with attentional generative adversarial networks.\\nIn Proceedings of the CVPR, pp. 1316–1324, 2018.\\nYang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C.\\nJust ask: Learning to answer questions from millions of'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='narrated videos. In Proceedings of the ICCV, pp. 1666–\\n1677, 2021.\\nYang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y.,\\nand Yu, D. Diffsound: Discrete diffusion model for text-\\nto-sound generation. IEEE ACM Trans. Audio Speech\\nLang. Process., 31:1720–1733, 2023.\\nYe, J., Hu, A., Xu, H., Ye, Q., Yan, M., Dan, Y., Zhao,\\nC., Xu, G., Li, C., Tian, J., Qi, Q., Zhang, J., and\\nHuang, F. mplug-docowl: Modularized multimodal large\\nlanguage model for document understanding.\\nCoRR,\\nabs/2307.02499, 2023a.\\nYe, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang,\\nJ., Hu, A., Shi, P., Shi, Y., Li, C., Xu, Y., Chen, H.,\\nTian, J., Qi, Q., Zhang, J., and Huang, F. mplug-owl:\\nModularization empowers large language models with\\nmultimodality. CoRR, abs/2304.14178, 2023b.\\nYin, Z., Wang, J., Cao, J., Shi, Z., Liu, D., Li, M., Sheng,\\nL., Bai, L., Huang, X., Wang, Z., Shao, J., and Ouyang,\\nW. LAMM: language-assisted multi-modal instruction-\\ntuning dataset, framework, and benchmark.\\nCoRR,\\nabs/2306.06687, 2023.\\nYoung, P., Lai, A., Hodosh, M., and Hockenmaier, J. From\\nimage descriptions to visual denotations: New similarity\\nmetrics for semantic inference over event descriptions.\\nTrans. Assoc. Comput. Linguistics, 2:67–78, 2014.\\nYu, B., Fu, C., Yu, H., Huang, F., and Li, Y. Unified lan-\\nguage representation for question answering over text,\\ntables, and images. CoRR, abs/2306.16762, 2023.\\nZeng, Z., Zhang, H., Lu, R., Wang, D., Chen, B., and Wang,\\nZ. Conzic: Controllable zero-shot image captioning by\\nsampling-based polishing. In Proceedings of the CVPR,\\npp. 23465–23476, 2023.\\nZhang, A., Fei, H., Yao, Y., Ji, W., Li, L., Liu, Z., and Chua,\\nT. Transfer visual prompt generator across llms. CoRR,\\nabs/2305.01278, 2023a.\\nZhang, B., Gu, S., Zhang, B., Bao, J., Chen, D., Wen, F.,\\nWang, Y., and Guo, B. Styleswin: Transformer-based\\nGAN for high-resolution image generation. In Proceed-\\nings of the CVPR, pp. 11294–11304, 2022.\\nZhang, D., Li, S., Zhang, X., Zhan, J., Wang, P., Zhou,\\nY., and Qiu, X. Speechgpt: Empowering large language\\nmodels with intrinsic cross-modal conversational abilities.\\nCoRR, abs/2305.11000, 2023b.\\nZhang, H., Li, X., and Bing, L. Video-llama: An instruction-\\ntuned audio-visual language model for video understand-\\ning. CoRR, abs/2306.02858, 2023c.\\nZhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D.,\\nand Sun, T. Llavar: Enhanced visual instruction tuning for\\ntext-rich image understanding. CoRR, abs/2306.17107,\\n2023d.\\n14\\nNExT-GPT: Any-to-Any Multimodal LLM\\nZhang, Z., Shi, Y., Yuan, C., Li, B., Wang, P., Hu,\\nW., and Zha, Z. Object relational graph with teacher-\\nrecommended learning for video captioning. In Proceed-\\nings of the CVPR, pp. 13275–13285, 2020.'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Zhao, B., Wu, B., and Huang, T. SVIT: scaling up visual\\ninstruction tuning. CoRR, abs/2307.04087, 2023a.\\nZhao, L., Yu, E., Ge, Z., Yang, J., Wei, H., Zhou, H., Sun,\\nJ., Peng, Y., Dong, R., Han, C., and Zhang, X. Chatspot:\\nBootstrapping multimodal llms via precise referring in-\\nstruction tuning. CoRR, abs/2307.09474, 2023b.\\nZhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., and Kang,\\nB. Bubogpt: Enabling visual grounding in multi-modal\\nllms. CoRR, abs/2307.08581, 2023c.\\nZhong, Y., Yang, J., Zhang, P., Li, C., Codella, N., Li,\\nL. H., Zhou, L., Dai, X., Yuan, L., Li, Y., and Gao, J.\\nRegionclip: Region-based language-image pretraining.\\nIn Proceedings of the CVPR, pp. 16772–16782, 2022.\\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny,\\nM. Minigpt-4: Enhancing vision-language understand-\\ning with advanced large language models.\\nCoRR,\\nabs/2304.10592, 2023.\\nZhu, M., Pan, P., Chen, W., and Yang, Y. DM-GAN: dy-\\nnamic memory generative adversarial networks for text-\\nto-image synthesis. In Proceedings of the CVPR, pp.\\n5802–5810, 2019.\\n15\\nNExT-GPT: Any-to-Any Multimodal LLM\\nA. Potential Limitation and Future work\\nAs future work, there are at least following four avenues to explore.\\n• i) Modalities & Tasks Expansion: Due to resource limitations, currently, our system supports input and output in four\\nmodalities: language, images, videos, and audio. Next, we plan to extend this to accommodate even more modalities\\n(e.g., web page, 3D vision, heat map, tables&figures) and tasks (e.g., object detection, segmentation, grounding, and\\ntracking), broadening the system’s applicability to become more universal.\\n• ii) LLM Variants: Currently, we have implemented the 7B Vicuna version of the LLM. Our next plans involve\\nincorporating various LLM types and sizes, allowing practitioners to choose the most suitable one for their specific\\nrequirements.\\n• iii) Multimodal Generation Strategies: While our system excels in generating content across modalities, the quality\\nof generative outputs can sometimes be limited by the capabilities of the diffusion model. It is very promising to\\nexplore the integration of retrieval-based approaches to complement the generative process, potentially improving the\\noverall system’s performance.\\n• iv) MosIT Dataset Expansion: Currently, our IT dataset has room for expansion. We intend to significantly increase\\nthe amount of annotated data, ensuring a more comprehensive and diverse set of instructions to further enhance the\\nMM-LLMs’ ability to understand and follow user prompts effectively.\\nB. Full Related Work\\nCross-modal Understanding and Generation\\nOur world is replete with multimodal information, wherein we continuously\\nengage in the intricate task of comprehending and producing cross-modal content. The AI community correspondingly\\nemerges varied forms of cross-modal learning tasks, such as Image/Video Captioning (Zeng et al., 2023; Dess`ı et al.,\\n2023; Milewski et al., 2020; Gu et al., 2023; Lin et al., 2022), Image/Video Question Answering (Yang et al., 2021; Xiao\\net al., 2022; Li et al., 2022; Yu et al., 2023; Anderson et al., 2018), Text-to-Image/Video/Speech Synthesis (Singer et al.,\\n2022; Hong et al., 2022; Voynov et al., 2023; Gal et al., 2022; Ding et al., 2021; Liu et al., 2023a; Huang et al., 2023a),\\nImage-to-Video Synthesis (Dorkenwald et al., 2021; Karras et al., 2023) and more, all of which have experienced rapid'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='advancements in past decades. Researchers have proposed highly effective multimodal encoders, intending to construct\\nunified representations encompassing various modalities. Meanwhile, owing to the distinct feature spaces of different\\nmodalities, it is essential to undertake modality alignment learning. Moreover, to generate high-quality content, a multitude\\nof strong-performing methods have been proposed, such as Transformer (Vaswani et al., 2017; Zhang et al., 2022; Ding\\net al., 2021; Ge et al., 2022), GANs (Liu et al., 2020; Brock et al., 2019; Xu et al., 2018; Zhu et al., 2019), VAEs (Vahdat &\\nKautz, 2020; Razavi et al., 2019), Flow models (Shibata et al., 2022; Bashiri et al., 2021) and the current state-of-the-art\\ndiffusion models (Hoogeboom et al., 2021; Qu et al., 2023b; Mou et al., 2023; Feng et al., 2022; Rombach et al., 2022).\\nEspecially, the diffusion-based methods have recently delivered a remarkable performance in a plethora of cross-modal\\ngeneration tasks, such as DALL-E (Ramesh et al., 2021), Stable Diffusion (Rombach et al., 2022). While all previous\\nefforts of cross-modal learning are limited to the comprehension of multimodal inputs only, CoDi (Tang et al., 2023) lately\\npresents groundbreaking development. Leveraging the power of diffusion models, CoDi possesses the ability to generate any\\ncombination of output modalities, including language, images, videos, or audio, from any combination of input modalities in\\nparallel. Regrettably, CoDi might still fall short of achieving human-like deep reasoning of input content, with only parallel\\ncross-modal feeding&generation.\\nMultimodal Large Language Models\\nLLMs have already made profound impacts and revolutions on the entire AI\\ncommunity and beyond. The most notable LLMs, i.e., OpenAI’s ChatGPT (OpenAI, 2022a) and GPT4 (OpenAI, 2022b),\\nwith alignment techniques such as instruction tuning (Ouyang et al., 2022; Li et al., 2023f; Zhang et al., 2023d; Liu et al.,\\n2023b) and reinforcement learning from human feedback (RLHF) (Stiennon et al., 2020), have demonstrated remarkable\\nlanguage understanding and reasoning abilities. And a series of open-source LLMs, e.g., Flan-T5 (Chung et al., 2022),\\nVicuna (Chiang et al., 2023), LLaMA (Touvron et al., 2023) and Alpaca (Taori et al., 2023), have greatly spurred advancement\\nand made contributions to the community (Zhu et al., 2023; Zhang et al., 2023a). Afterward, significant efforts have been\\nmade to construct LLMs dealing with multimodal inputs and tasks, leading to the development of MM-LLMs.\\nOn the one hand, most of the researchers build fundamental MM-LLMs by aligning the well-trained encoders of various\\nmodalities to the textual feature space of LLMs, so as to let LLMs perceive other modal inputs (Huang et al., 2023c; Zhu\\net al., 2023; Su et al., 2022; Koh et al., 2023). For example, Flamingo (Alayrac et al., 2022) uses a cross-attention layer to\\nconnect a frozen image encoder to the LLMs. BLIP-2 (Li et al., 2023c) employs a Q-Former to translate the input image\\n16\\nNExT-GPT: Any-to-Any Multimodal LLM\\nqueries to the LLMs. LLaVA (Liu et al., 2023b) employs a simple projection scheme to connect image features into the word\\nembedding space. There are also various similar practices for building MM-LLMs that are able to understand videos (e.g.,\\nVideo-Chat (Li et al., 2023d) and Video-LLaMA (Zhang et al., 2023c)), audios (e.g., SpeechGPT (Zhang et al., 2023b)), etc.\\nProfoundly, PandaGPT (Su et al., 2023) achieves a comprehensive understanding of six different modalities simultaneously\\nby integrating the multimodal encoder, i.e., ImageBind (Girdhar et al., 2023).'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Nevertheless, these MM-LLMs all are subject to the limitation of only perceiving multimodal data, without generating\\ncontent in arbitrary modalities. To achieve LLMs with both multimodal input and output, some thus explore employing LLMs\\nas decision-makers, and utilizing existing off-the-shelf multimodal encoders and decoders as tools to execute multimodal\\ninput and output, such as Visual-ChatGPT (Wu et al., 2023), HuggingGPT (Shen et al., 2023), and AudioGPT (Huang et al.,\\n2023b). As aforementioned, passing messages between modules with pure texts (i.e., LLM textual instruction) under the\\ndiscrete pipeline scheme will inevitably introduce noises. Also lacking comprehensive tuning across the whole system\\nsignificantly limits the efficacy of semantics understanding. Our work takes the mutual benefits of both the above two types,\\ni.e., learning an any-to-any MM-LLM in an end-to-end manner.\\nC. Implementation Details\\nC.1. Detailed Input Projection Layer\\nThrough multimodal encoder, we can obtain patch-level multimodal tokens, denoting as X∗= {x∗\\ni }N ∗\\ni=1, where ∗∈{i, a, v}\\nrepresents image, audio, and video, respectively. For brevity, we eschew modal-specific notation. Differing from the existing\\nworks that directly embed multimodal tokens into LLMs by a linear projection layer, we propose a multi-stage grouping\\nmechanism, where patch-level tokens are grouped into concept-level tokens to facilitate the subsequent cross-modal\\ninteraction. Formally, we apply L grouping stages, and in each stage, we randomly initialize Ml learnable concept tokens\\nCl = {cj}Ml\\nj . Then, we concatenate input features Xl and Cl together and then input them into a transformer layers:\\nˆCl, ˆ\\nXl = Transformer([Cl; Xl]), where X1 = X, and [; ] denotes the concatenation operator. Within l grouping block,\\nwe group the updated Ml concept tokens ˆ\\nXl into Ml+1 new concept tokens ˆ\\nXl+1 based on the feature similarity.\\nSpecifically, we firstly compute a similarity matrix Al between\\nˆCl and\\nˆ\\nXl via a Gumbel-softmax:\\nAl\\n=\\nSoftmax((Norm( ˆCl)Norm( ˆ\\nXl) + G)/τ), where G are i.i.d random samples drawn from the Gumbel(0, 1) distribution,\\nand τ is the learnable significance coefficient to assist to find a more suitable assign boundary. We compute the group to\\nassign a concept token by taking the one-hot operation on the argmax over all the groups. Since the one-hot assignment\\noperation via argmax is not differentiable, we instead use the straight-through trick to compute the assignment matrix as\\nˆ\\nAl = Onehot(Argmax(Al)) + Al −Sg(Al), where Sg(.) is the stop gradient operator. Finally, we integrate the features to\\nupdated concept tokens: Xl+1 = ˆCl + MLP( ˆ\\nAl, ˆ\\nXl). After L stages grouping, we can obtain ML concept tokens XL,\\nwhich are then fed into the LLM for perception and reasoning.\\nC.2. Model Training\\nFor NExT-GPT model training, we consider a three-stage learning process:\\n• Stage-1: Encoding-side Alignment Learning. As discussed in Section §4.1, we bridge the alignment to perform\\nthe caption generation task. The cross-entropy is employed as the loss function. During training, we only keep the\\ninput projection layer trainable while the other part of NExT-GPT is frozen. We employ Adam (Kingma & Ba, 2015)\\noptimizer to update the parameters. This stage can be understood as training a compatible multimodal tokenizer for the\\nfrozen LLM.\\n• Stage-2: Decoding-side Alignment Learning. The output projection layer adopts a transformer-based architecture\\ncharacterized by a hidden size of 512, 4 attention heads, 4 encoder layers, and 4 decoder layers. Additionally, the\\ndropout ratio is set as 0.1. The optimization process for the three output projection layers involves a combination of'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='three training objectives: cross-entropy focusing on the generated signal tokens, l2-distance measuring the alignment\\nbetween the representation of signal tokens and captions, and conditional latent denoising loss, as shown in Section\\n§4.2. We employ the Adam optimizer for this stage, with only the parameters of the output projection layers being\\nlearnable, while others remain frozen.\\n• Stage-3: End-to-end Instruction-Tuning. In this stage. we train the whole NExT-GPT using instruction-following\\ndatasets, as enumerated in Section §5.2. We incorporate LoRA to fine-tune the weights of the LLM. Moreover, both the\\ninput and output projection layers are trainable. The training objectives include two parts: 1) cross-entropy between the\\n17\\nNExT-GPT: Any-to-Any Multimodal LLM\\ngenerated and gold response, 2) generation loss. The Adam optimizer is applied to update the learnable parameters.\\nC.3. Detailed Dataset\\nHere, we enumerate the datasets employed for training and fine-tuning NExT-GPT:\\n• ‘Text-X’ Pair Dataset.\\n– CC3M (Sharma et al., 2018): contains over 3 million images accompanied by diverse styles of natural-language\\ndescriptions.\\n– COCO-caption (Lin et al., 2014): is a large-scale image-text pair dataset which is taken as image captioning, or\\ntext-to-image generation task benchmark.\\n– WebVid-2M (Bain et al., 2021): is a large-scale dataset of short videos with textual description sourced from\\nstock footage sites.\\n– AudioCaps (Kim et al., 2019): with 46K audio-text pairs derived from the AudioSet (Gemmeke et al., 2017)\\ndataset.\\n• ‘Text’ →‘Text’ Instruction Datasets.\\n– Cleaned-Alpaca1: is a textual instruction dataset used to train the Alpaca LLM (Large Language Model).\\n• ‘Text+X’ →‘Text’ Instruction Datasets.\\n– LLaVA-150K (Liu et al., 2023b): is a set of GPT-generated multimodal instruction-following data. It is\\nconstructed for visual instruction tuning and for building large multimodal towards GPT-4 vision/language\\ncapability.\\n– VideoChat (Li et al., 2023d): comprising thousands of videos paired with detailed dataset textual descriptions\\nand conversations generated using dense captions fed to ChatGPT in temporal order.\\n• Modality-switching Dataset\\n– MosIT: encompasses a wide range topic of dialogues between ‘Human’ and ‘Machine’. Each dialogue includes\\n3-7 turns (i.e., QA pairs), where the ‘Human’-‘Machine’ interactions should involve multiple modalities at either\\nthe input or output side, and switch the modalities alternately.\\nC.4. Multimodal IT Datasets Comparison\\nHere, we compare the existing multimodal instruction tuning (IT) datasets, as detailed in Table 6. As can be seen, the\\nresponse modality of the existing IT datasets is merely limited to text. In this work, we leverage GPT-4 to generate a T2M\\nIT dataset, comprising 15k instances, which serves as a foundation for instructing the model to generate responses in other\\nmodalities, such as image, video, and audio. Furthermore, we construct a modality-switching IT dataset with 5k instances,\\nnamed MosIT. This dataset is designed to emulate the human-machine complex interaction featuring diverse and dynamic\\nshifts in modalities within both inputs and outputs.\\nC.5. Training Recipes\\nIn Table 7, we list the detailed hyper-parameters setting at three stages.\\nC.6. Inference Process\\nIn Figure 7 we further illustrate the inference procedure of NExT-GPT. Given certain user inputs of any combination of\\nmodalities, the corresponding modal encoders, and projectors transform them into feature representations and pass them to\\nLLM2. Then, LLM decides what content to generate, i.e., textual tokens, and modality signal tokens. If LLM identifies a\\ncertain modality content (except language) to be produced, a special type of token (Koh et al., 2023) will be output indicating'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='the activation of that modality; otherwise, no special token output means deactivation of that modality.\\n1https://github.com/gururise/AlpacaDataCleaned\\n2Except the text inputs, which will be directly fed into LLM.\\n18\\nNExT-GPT: Any-to-Any Multimodal LLM\\nDataset\\nData Source\\nIn→Out Modality\\nApproach\\nMulti-turn Reason #Img/Vid/Aud #Dialog Turn. #Instance\\n▶Existing data\\nMiniGPT-4 (Zhu et al., 2023)\\nCC, CC3M\\nT+I→T\\nAuto\\n✗\\n134M/-/-\\n1\\n5K\\nStableLLaVA (Li et al., 2023f)\\nSD\\nT+I→T\\nAuto+Manu.\\n✗\\n126K/-/-\\n1\\n126K\\nLLaVA (Zhang et al., 2023d)\\nCOCO\\nT+I→T\\nAuto\\n✓\\n81K/-/-\\n2.29\\n150K\\nSVIT (Zhao et al., 2023a)\\nMS-COCO, VG\\nT+I→T\\nAuto\\n✓\\n108K/-/-\\n5\\n3.2M\\nLLaVAR (Zhang et al., 2023d)\\nCOCO, CC3M , LAION\\nT+I→T\\nLLaVA+Auto\\n✓\\n20K/-/-\\n2.27\\n174K\\nVideoChat (Li et al., 2023d)\\nWebVid\\nT+V→T\\nAuto\\n✓\\n-/8K/-\\n1.82\\n11K\\nVideo-ChatGPT (Maaz et al., 2023)\\nActivityNet\\nT+V→T\\nInherit\\n✗\\n-/100K/-\\n1\\n100K\\nVideo-LLaMA (Zhang et al., 2023c)\\nMiniGPT-4, LLaVA, VideoChat\\nT+I/V→T\\nAuto\\n✓\\n81K/8K/-\\n2.22\\n171K\\nInstructBLIP (Dai et al., 2023)\\nMultiple\\nT+I/V→T\\nAuto\\n✗\\n-\\n-\\n∼1.6M\\nMIMIC-IT (Li et al., 2023b)\\nMultiple\\nT+I/V→T\\nAuto\\n✗\\n8.1M/502K/-\\n1\\n2.8M\\nPandaGPT (Su et al., 2023)\\nMiniGPT-4, LLaVA\\nT+I→T\\nInherit\\n✓\\n81K/-/-\\n2.29\\n160K\\nMGVLID (Zhao et al., 2023b)\\nMultiple\\nT+I+B→T\\nAuto+Manu.\\n✗\\n108K/-/-\\n-\\n108K\\nM3IT (Li et al., 2023e)\\nMultiple\\nT+I/V/B→T\\nAuto+Manu.\\n✗\\n-/-/-\\n1\\n2.4M\\nLAMM (Yin et al., 2023)\\nMultiple\\nT+I+PC→T\\nAuto+Manu.\\n✓\\n91K/-/-\\n3.27\\n196k\\nBuboGPT (Zhao et al., 2023c)\\nClotho , VGGSS\\nT+A/(I+A)→T\\nAuto\\n✗\\n5k/-/9K\\n-\\n9K\\nmPLUG-DocOwl (Ye et al., 2023a)\\nMultiple\\nT+I/Tab/Web→T\\nInherit\\n✗\\n-\\n-\\n-\\n▶In this work\\nT2M\\nWebvid , CC3M , AudioCap\\nT→T+I/A/V\\nAuto\\n✗\\n5K/5K/5K\\n1\\n15K\\nMosIT\\nYoutube, Google, Flickr, Midjourney, etc. T+I+A+V→T+I+A+V Auto+Manu.\\n✓\\n4K/4K/4K\\n4.8\\n5K\\nTable 6. Summary and comparison of existing datasets for multimodal instruction tuning. T: text, I: image, V: video, A: audio, B: bounding box, PC: point cloud, Tab: table, Web: web page.\\n19'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='NExT-GPT: Any-to-Any Multimodal LLM\\nTable 7. Training recipes for NExT-GPT. The three training stages are introduced in Section C.2. Stage-1: Encoding-side Alignment Learning, Stage-2:\\nDecoding-side Alignment Learning, Stage-3: End-to-end Instruction Tuning.\\nConfiguration\\nStage-1\\nStage-2\\nStage-3\\nOptimizer\\nAdam\\nAdam\\nAdam\\nLearning Rate\\n0.0004\\n0.0004\\n0.0005\\nWeight Decay\\n0.001\\n0.001\\n0.001\\nTraining Epochs\\n1\\n1\\n1\\nWarmup Ratio\\n0.1\\n0.1\\n0.1\\nLearning Rate Scheduler\\nLinear\\nLinear\\nLinear\\nBatch Size Per GPU\\n18\\n8\\n4\\nMaximum Token Length\\n512\\n512\\n512\\nUnfreeze LLM\\nTraining Data\\nDataset\\nCC3M\\nCC3M\\nLLaVA-150K, VideoChat\\nWebVid\\nWebVid\\ncleaned-Alpaca\\nAudioCaps\\nAudioCaps\\nText→Text+X, MosIT\\nTable 8. Text-to-image generation results on COCO-\\ncaption (Lin et al., 2014).\\nMethod\\nFID (↓)\\nCogView (Ding et al., 2021)\\n27.10\\nGLIDE (Nichol et al., 2022)\\n12.24\\nCoDi (Tang et al., 2023)\\n11.26\\nSD (Rombach et al., 2022)\\n11.21\\nNExT-GPT\\n11.18\\nTable 9. Text-to-video generation results (zero-shot) on MSR-VTT\\n(Xu et al., 2016).\\nMethod\\nFID (↓) CLIPSIM (↑)\\nCogVideo (Hong et al., 2022)\\n23.59\\n26.31\\nMakeVideo (Singer et al., 2022)\\n13.17\\n30.49\\nLatent-VDM (Rombach et al., 2022)\\n14.25\\n27.56\\nLatent-Shift (An et al., 2023)\\n15.23\\n27.73\\nCoDi (Tang et al., 2023)\\n—\\n28.90\\nNExT-GPT\\n12.69\\n31.97\\nTable 10. Text-to-audio generation results on Audio-\\nCaps (Kim et al., 2019).\\nMethod\\nFD (↓)\\nIS (↑)\\nDiffSound (Yang et al., 2023)\\n47.68\\n4.01\\nAudioLDM-S (Liu et al., 2023a)\\n29.48\\n6.90\\nAudioLDM-L (Liu et al., 2023a)\\n23.31\\n8.13\\nCoDi (Tang et al., 2023)\\n22.90\\n8.77\\nNExT-GPT\\n23.25\\n8.67\\nTable 11.\\nAudio-to-text generation (audio captioning) results on\\nAudioCaps (Kim et al., 2019).\\nMethod\\nSPIDEr\\nCIDEr\\nAudioCaps (Kim et al., 2019)\\n0.369\\n0.593\\nBART (Gontier et al., 2021)\\n0.465\\n0.753\\nAL-MixGen (Kim et al., 2022)\\n0.466\\n0.755\\nCoDi (Tang et al., 2023)\\n0.480\\n0.789\\nNExT-GPT\\n0.534\\n0.807\\nD. Additional Experiments\\nD.1. Additional Multimodal Comprehension and Generation Results\\nFirstly, we examine the synthesis quality of the image, video, or audio conditioned on text compared with the non-LLM-\\nbased methods. Table 8, 10, 9 present the comparisons between ours and some state-of-the-art systems. On text-to-image\\nand text-to-audio generation tasks, NExT-GPT shows a nice performance on par with that of the best-performing baselines.\\nNotably, under the zero-shot setting, NExT-GPT shows a significant superiority in video generation conditioning on text,\\ndemonstrating the remarkable generalization capability of NExT-GPT.\\nSecondly, we evaluate the NExT-GPT on the tasks of textual caption generation to test the semantic understanding capability'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content=\"w.r.t. image, video, or audio. The results on different tasks are shown in Table 12, 11, and 13. Significantly, NExT-GPT\\nmostly achieves much better performance on the X-to-text generation than that of the CoDi baseline, owing to the direct\\ngeneration of texts from LLM, which is inherently expertized by the LLM.\\nThirdly, we test our model on text-conditioned modal editing tasks. Table 14, 16 and 15 show the performances on different\\ntasks. Compared with the above two types of tasks, although NExT-GPT did not demonstrate superior performance on the\\ntext-conditioned modal editing tasks, it still shows competitive performance.\\n20\\nNExT-GPT: Any-to-Any Multimodal LLM\\nLLM\\nImage Input \\nProjection\\nImage \\nEncoder\\nI am so into summer, especially the \\nsea; I hope I can go to the seaside \\nto have some fun.\\nImage \\nDiffusion \\nImage Output \\nProjection\\nAudio Input \\nProjection\\nAudio \\nEncoder\\nVideo \\nEncoder\\nVideo Input \\nProjection\\nAudio \\nDiffusion\\nVideo \\nDiffusion\\nAudio Output \\nProjection\\nVideo Output \\nProjection\\nAbsolutely! There are so \\nmany activities to enjoy \\nby the sea, like beach \\nvolleyball.\\n But I'm really interested \\nin trying out surfing. I \\nthink it's super cool. It \\nwould be even better if I \\ncould create a vlog to \\nshowcase my progress.\\nLLM\\nImage Input \\nProjection\\nImage \\nEncoder\\nImage \\nDiffusion \\nImage Output \\nProjection\\nAudio Input \\nProjection\\nAudio \\nEncoder\\nVideo \\nEncoder\\nVideo Input \\nProjection\\nAudio \\nDiffusion\\nVideo \\nDiffusion\\nAudio Output \\nProjection\\nVideo Output \\nProjection\\n Vlog content can be \\nquite diverse. Here's a \\nreference \\nImg. Sig. Tok. Rep.\\nVid. Sig. Tok. Rep.\\nLLM\\nImage Input \\nProjection\\nImage \\nEncoder\\nImage \\nDiffusion \\nImage Output \\nProjection\\nAudio Input \\nProjection\\nAudio \\nEncoder\\nVideo \\nEncoder\\nVideo Input \\nProjection\\nAudio \\nDiffusion\\nVideo \\nDiffusion\\nAudio Output \\nProjection\\nVideo Output \\nProjection\\nFor the cover, it should \\ndefinitely feature you \\nsurfing. As for the \\nmusic, I recommend \\nsomething lively, like \\nthis?\\nI am so into summer, especially the \\nsea; I hope I can go to the seaside \\nto have some fun.\\nAbsolutely! There are so \\nmany activities to enjoy \\nby the sea, like beach \\nvolleyball.\\nAbsolutely! There are so \\nmany activities to enjoy \\nby the sea, like beach \\nvolleyball.\\n But I'm really interested \\nin trying out surfing. I \\nthink it's super cool. It \\nwould be even better if I \\ncould create a vlog to \\nshowcase my progress.\\nI am so into summer, especially the \\nsea; I hope I can go to the seaside \\nto have some fun.\\n Vlog content can be \\nquite diverse. Here's a \\nreference \\nCreating a vlog requires a cover and \\nmusic. What do you think would be \\na good style?\\nImg. Sig. Tok. Rep.\\nAud. Sig. Tok. Rep.\\nLLM\\nImage Input \\nProjection\\nImage \\nEncoder\\nImage \\nDiffusion \\nImage Output \\nProjection\\nAudio Input \\nProjection\\nAudio \\nEncoder\\nVideo \\nEncoder\\nVideo Input \\nProjection\\nAudio \\nDiffusion\\nVideo \\nDiffusion\\nAudio Output \\nProjection\\nVideo Output \\nProjection\\nOf course! To begin learning surfing, \\nyou should first find a good surfing \\ninstructor to learn basic skills and \\nsafety knowledge. Additionally ...\\nAbsolutely! There are so \\nmany activities to enjoy \\nby the sea, like beach \\nvolleyball.\\n But I'm really interested \\nin trying out surfing. I \\nthink it's super cool. It \\nwould be even better if I \\ncould create a vlog to\"),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content=\"showcase my progress.\\nI am so into summer, especially the \\nsea; I hope I can go to the seaside \\nto have some fun.\\n Vlog content can be \\nquite diverse. Here's a \\nreference \\nCreating a vlog requires a cover and \\nmusic. What do you think would be \\na good style?\\nFor the cover, it should \\ndefinitely feature you \\nsurfing. As for the \\nmusic, I recommend \\nsomething lively, like \\nthis?\\nCan you provide me with some  \\nlearning tips? I can't wait to start \\nlearning.\\nFigure 7. NExT-GPT inference process. Grey colors denote the deactivation of the modules.\\n21\\nNExT-GPT: Any-to-Any Multimodal LLM\\nTable 12. Image-to-text generation (image captioning) results on COCO-\\ncaption (Lin et al., 2014).\\nMethod\\nB@4\\nMETEOR\\nCIDEr\\nOscar (Li et al., 2020)\\n36.58\\n30.4\\n124.12\\nBLIP-2 (Li et al., 2023c)\\n43.7\\n—\\n145.8\\nOFA (Wang et al., 2022b)\\n44.9\\n32.5\\n154.9\\nCoDi (Tang et al., 2023)\\n40.2\\n31.0\\n149.9\\nNExT-GPT\\n45.1\\n34.1\\n158.3\\nTable 13. Video-to-text generation (video captioning) results on MSR-\\nVTT (Xu et al., 2016).\\nMethod\\nB@4 METEOR\\nORG-TRL (Zhang et al., 2020)\\n43.6\\n28.8\\nGIT (Wang et al., 2022a)\\n54.8\\n33.1\\nmPLUG-2 (Xu et al., 2023)\\n57.8\\n34.9\\nCoDi (Tang et al., 2023)\\n52.1\\n32.5\\nNExT-GPT\\n58.8\\n39.6\\nTable 14. Text+image-to-image generation (text-conditioned image editing)\\nresults on COCO-caption (Lin et al., 2014).\\nMethod\\nObject\\nBackground\\nCLIP (↑) FID (↓) CLIP (↑) FID (↓)\\nPTP (Hertz et al., 2023)\\n30.33\\n9.58\\n31.55\\n13.92\\nBLDM (Avrahami et al., 2023)\\n29.95\\n6.14\\n30.38\\n20.44\\nDiffEdit (Couairon et al., 2023)\\n29.30\\n3.78\\n26.92\\n1.74\\nPFB-Diff (Huang et al., 2023d)\\n30.81\\n5.93\\n32.25\\n13.77\\nNExT-GPT\\n29.32\\n6.62\\n27.31\\n14.27\\nTable 15.\\nText+video-to-video generation (text-conditioned video\\nediting) results on DAVIS (Perazzi et al., 2016).\\nMethod\\nCLIP-T CLIP-I\\nCogVideo (Hong et al., 2022)\\n0.2391 0.9064\\nTuneVideo (Wu et al., 2022)\\n0.2758 0.9240\\nSDEdit (Meng et al., 2022)\\n0.2775 0.8731\\nPix2Video (Ceylan et al., 2023) 0.2891 0.9767\\nNExT-GPT\\n0.2684 0.9647\\nTable 16.\\nText+audio-to-audio generation (text-conditioned speech editing) results on VCTK (Veaux et al., 2017).\\nMethod\\nMCD (↓)\\nCampNet (Wang et al., 2022c)\\n0.380\\nMakeAudio (Huang et al., 2023a)\\n0.375\\nAudioLDM-L (Liu et al., 2023a)\\n0.349\\nNExT-GPT\\n0.300\\nD.2. Human Evaluation on Complex Any-to-any QA\\nWe also carry out evaluation on some more scenarios where there are complicated cross-modal interactions between inputs\\nand outputs. We mainly compare the model performance for the settings with different modality conversions. As no standard\"),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='benchmark can be leveraged, here we adopt human evaluation. We ask several evaluators to score the performance of\\nNExT-GPT on a scale from 1 to 10. Figure 8 shows the comparisons. We find NExT-GPT is more competent in producing\\nimages, compared with the generations on videos and audio. Also generating mixed combinations of multimodal content is\\nslightly inferior to the generation of single-modal content, due to the complexity of the latter.\\nT\\nI+A\\nT\\nT+V\\nT+I\\nV\\nT+I\\nA\\nT+V\\nV\\nT+V\\nI+A\\nT+A\\nI\\nT+A\\nI+A\\nT+A\\nV\\nT+A+I\\nV\\nT+A+I\\nI\\nT+A+I+V\\nI\\n0\\n2\\n4\\n6\\n8\\n10\\nPerformance\\nFigure 8. Comparative performance of NExT-GPT on various complex cross-modal conversions.\\nD.3. Case Study on Pipeline-style vs. End-to-end Unification\\nWe earlier have elaborated on the difference as well as the necessity of building a unified any-to-any multimodal LLM in an\\nend-to-end manner, compared with the existing pipeline-style systems that generate intermediate captions and then pass to\\n22\\nNExT-GPT: Any-to-Any Multimodal LLM\\nthe downstream tools (e.g., diffusion models for generation). The cascade process inevitably introduces noise and propagates\\nerrors. Meanwhile, the entire system only leverages existing pre-trained tools for inference, whereas, without end-to-end\\nupdating throughout the whole system, the capability to more accurately interpret complex user instructions and generate\\ncontent will be compromised. Here we add a few illustrations, where we make comparisons with these pipeline-style systems:\\n1) Visual-ChatGPT and HuggingGPT, which are existing systems that have free open access; 2) NExT-GPT variant with\\ncaptions as the messenger (which we mark as NExT-GPT-caption). To implement NExT-GPT-caption, the captions directly\\ngenerated by LLM will be fed into the following generation models, instead of using the soft representations of the signal\\ntokens. As Visual-ChatGPT only supports image generation, we here consider the evaluation on the Text-to-Text&Image\\nsetting.\\nimage/00e38ab0.png.\\nFigure 9. Illustration of case study, image generation from a simple instruction on Visual-ChatGPT, NExT-GPT-caption, and NExT-GPT.\\nFigure 9 presents the case of image generation from a simple input user instruction; while Figure 10 and 11 present two\\ncases of image generation from comparatively complex input user instructions. On the simple one, all generated image\\ncontent from both pipeline-style and end-to-end (ours) systems seem correct and coincide with the input prompt. However,\\nwhen handling complex instructions, as seen in Figure 10 and 11, the generated image content can be wrong and biased\\nto the user’s intention. The problems are rooted in the core of different modalities, i.e., there are inherent gaps between\\nlanguage and visual modalities that cannot be eliminated. Here are two representative attributes: the numeration of vision\\n(cf. Figure 10) and the visual-spatial relational semantics (cf. Figure 11), which could be hard to (or even cannot) be\\nexpressed by the intermediate captions conveniently. Utilizing textual captions as intermediate representations runs the\\nrisk of overlooking these modality-specific features when expressing non-linguistic (e.g., visual) modalities solely through\\nlanguage.\\nBy the way, we kindly note a fact that, with the intermediate captions produced from the pipeline-style systems in Figure 10\\nand 11, the Stable Diffusion model just has difficulty in accurately understanding the vision numeration and visual-spatial\\nrelation and generating correct answers, i.e., they are the problems inherent to the Stable Diffusion model itself, and Stable\\nDiffusion alone is tricky to overcome. Most recent work tries to solve this issue by integrating the vision-specific features\\ninto the Stable Diffusion (Feng et al., 2023; Qu et al., 2023a) via additional feature engineering. But, in our NExT-GPT\\nwith an end-to-end solution, the implicit modality signal token embeddings that carry rich modality-specific features of'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='non-linguistic will be naturally encoded and passed to the downstream modules (e.g., Stable Diffusion), without any further\\nexternal effort.\\nD.4. Example Demonstrations\\nFigure 12, 13, 14, 15, 16, 17, 18 and 19 show several real examples produced by NExT-GPT.\\n23\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 10. Illustration of image generation from a complex instruction on HuggingGPT, NExT-GPT-caption, and NExT-GPT. In this case, the numeration\\nunderstanding of vision has been wrong by pipeline-style methods.\\n24\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 11. Illustration of image generation from another complex instruction on HuggingGPT, NExT-GPT-caption, and NExT-GPT. In this case, the\\nunderstanding of visual-spatial relational semantics has been wrong by pipeline-style methods.\\n25\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 12. Example of Text+Image →Text+Audio.\\n26\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 13. Example of Text →Text+Image+Video+Audio.\\n27\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 14. Example of Text+Image →Text+Image+Video+Audio.\\n28\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 15. Example of Text+Video →Text+Image.\\n29\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 16. Example of Text+Audio →Text+Image+Video.\\n30\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 17. Example of Text+Video →Text+Audio.\\n31\\nNExT-GPT: Any-to-Any Multimodal LLM\\nFigure 18. Example of Text →Text+Video.\\nFigure 19. Example of Text →Text+Image.\\n32')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VectorDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to load libllamamodel-mainline-cuda-avxonly.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "Failed to load libllamamodel-mainline-cuda.so: dlopen: libcudart.so.11.0: cannot open shared object file: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=NomicEmbeddings(model=\"nomic-embed-text-v1.5\", inference_mode=\"local\"),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Retrieval Grader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_llm = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_605056/1271020123.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n\n",
    "    If the document contains keywords related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "question = \"llm\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Name\\nParam\\nName\\nParam\\nText\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\n—\\nImage\\nVicuna\\n7B\\nTransformer\\n31M\\nSD\\n1.3B\\nAudio\\n(LoRA\\n33M\\n)\\nTransformer\\n31M\\nAudioLDM\\n975M\\nVideo\\nImageBind\\n1.2B\\nGrouping\\n28M\\nTransformer\\n32M\\nZeroscope\\n1.8B\\nrepresentations are mapped into language-like representa-\\ntions that are comprehensible to the LLM.\\nLLM Understanding and Reasoning Stage\\nAn LLM\\nis used as the core agent of NExT-GPT. Technically, we\\nemploy the Vicuna (7B-v0) (Chiang et al., 2023), which is\\nthe open-source text-based LLM that is widely used in the\\nexisting MM-LLMs (Su et al., 2023; Zhang et al., 2023c).\\nLLM takes as input the representations from different modal-\\nities and carries out semantic understanding and reasoning\\nover the inputs. It outputs: 1) the textual responses directly,\\nand 2) signal tokens of each modality that serve as instruc-\\ntions to dictate the decoding layers on whether to generate\\nmultimodal contents and what content to produce if yes.\\nMultimodal Generation Stage\\nReceiving the multimodal\\nsignals with specific instructions from LLM (if any), the\\nTransformer-based output projection layers map the sig-\\nnal token representations into the ones that are understand-\\nable to the following multimodal decoders. Technically,\\nwe employ the current off-the-shelf latent conditioned dif-\\nfusion models of different modal generations, i.e., Stable\\nDiffusion (SD-v1.5) for image synthesis (Rombach et al.,\\n2022), Zeroscope (v2-576w) for video synthesis (Cer-\\nspense, 2023), and AudioLDM (l-full) for audio synthe-\\nsis (Liu et al., 2023a). After a projection layer, the signal\\nrepresentations are fed into the conditioned diffusion mod-\\nels for content generation. In Table 1 we summarize the\\noverall system configurations. It is noteworthy that in the\\nentire system, only the input and output projection layers\\nof lower-scale parameters (compared with the overall huge\\ncapacity framework) are required to be updated during the\\nfollowing learning, with all the rest of the encoders and de-\\ncoders frozen. This amounts to, 155M(=28+33+31+31+32)\\n/ [155M + 12.275B(=1.2+7+1.3+1.8+0.975)], or only 1%\\nof parameters need to be updated. This is also one of the\\nkey advantages of our MM-LLM.\\n4. Lightweight Multimodal Alignment\\nLearning\\nTo bridge the gap between the feature space of different\\nmodalities, and ensure fluent semantics understanding of\\ndifferent inputs, it is essential to perform alignment learning\\nfor NExT-GPT. Since we design the loosely-coupled system\\nwith mainly three tiers, we only need to update the two\\nprojection layers at the encoding side and decoding side.\\n4.1. Encoding-side LLM-centric Multimodal Alignment\\nMost\\nexisting\\nMM-LLMs\\nadopt\\nthe\\nTransformer-\\narchitectured multimodal encoders and generate patch-level\\ngrid features (e.g., for image, audio or video).\\nThey\\ntransform the multimodal features to be understandable\\nto the core LLM by projecting them into the text feature\\nspace straightforwardly via linear layers. However, we note\\nthat the patch-based feature units might not best coincide\\nwith the intricate textual token semantics, as intuitively\\nthe language tokens always encapsulate separate concepts.\\nThis may result in suboptimal information perception\\n(Zhong et al., 2022) in MM-LLMs.\\nThus, inspired by\\n(Xu et al., 2022), we design a type of learnable concept\\ntokens to hierarchically aggregate the grid-level features\\ninto semantic concept tokens via a grouping mechanism,\\nand then the conceptual representation is fed into LLM.\\nTo accomplish the alignment, we adopt an ‘X-to-text’ gen-'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='DREAMLLM (Dong et al., 2023), GILL (Koh et al., 2023),\\nSEED (Ge et al., 2023). Notably, these models are confined\\nto generating interleaved texts and images. We emphasize\\nthat natural human cognition and communication indispens-\\nably require seamless transitions between any modalities\\nof information. This makes the exploration of any-to-any\\nMM-LLMs critical, i.e., the ability to accept inputs in any\\nmodality and deliver responses in any appropriate modality.\\nCertain efforts have been made to mimic the human-like\\nany-to-any modality conversion. Lately, CoDi (Tang et al.,\\n2023) has made strides in implementing the capability of\\nsimultaneously processing and generating arbitrary combi-\\nnations of modalities; however, it lacks the reasoning and\\ndecision-making prowess of LLMs as its core, and is also\\nlimited to simple paired content generation. On the other\\nhand, some efforts, e.g., Visual-ChatGPT (Wu et al., 2023)\\n1\\narXiv:2309.05519v3  [cs.AI]  25 Jun 2024\\nNExT-GPT: Any-to-Any Multimodal LLM\\nLLM\\nAudio \\nDiffusion\\nVideo \\nDiffusion\\nLLM-based Semantic \\nUnderstanding\\nInstruction-following \\nAlignment\\nImage \\nDiffusion \\nText\\nImage\\nAudio\\nVideo\\nImage Output \\nProjection\\nAudio Output \\nProjection\\nVideo Output \\nProjection\\nImage Input \\nProjection\\nImage \\nEncoder\\nAudio Input \\nProjection\\nAudio \\nEncoder\\nVideo \\nEncoder\\nVideo Input \\nProjection\\n...\\nMore modalities\\n...\\nMultimodal Input \\nEncoding \\nMultimodal Output \\nGeneration\\nLLM-centric \\nAlignment\\nFigure 1. By connecting LLM with multimodal adaptors and diffusion decoders, NExT-GPT achieves universal multimodal understanding and any-to-any\\nmodality input and output.\\nand\\nrepresent the frozen and trainable modules, respectively.\\nand HuggingGPT (Shen et al., 2023), have sought to com-\\nbine LLMs with external tools to achieve approximately\\nthe ‘any-to-any’ multimodal understanding and generation.\\nUnfortunately, these systems suffer from critical challenges\\ndue to their complete pipeline architecture. First, the in-\\nformation transfer between different modules is entirely\\nbased on discrete texts produced by the LLM, where the\\ncascading process inevitably introduces noise and propa-\\ngates errors. More critically, the entire system leverages\\nexisting pre-trained tools for inference only. Due to the lack\\nof overall end-to-end training, the capabilities of content\\nunderstanding and multimodal generation can be very lim-\\nited, especially in interpreting intricate and implicit user\\ninstructions. In a nutshell, there is a compelling need to\\nconstruct an end-to-end MM-LLM of arbitrary modalities.\\nIn pursuit of this goal, we present NExT-GPT, an any-to-\\nany MM-LLM designed to seamlessly handle input and\\noutput in any combination of four modalities: text, image,\\nvideo, and audio. As depicted in Figure 1, NExT-GPT com-\\nprises three tiers. First, we leverage established encoders\\nto encode inputs in various modalities, where these repre-\\nsentations are projected into language-like representations\\ncomprehensible to LLM through a projection layer. Second,\\nwe harness an existing open-sourced LLM as the core to\\nprocess input information for semantic understanding and\\nreasoning. The LLM not only directly generates text tokens\\nbut also produces unique ‘modality signal’ tokens that serve\\nas instructions to dictate the decoding layers on whether\\nand what modal content to output correspondingly. Third,\\nafter projection, the produced multimodal signals with spe-\\ncific instructions are routed to different encoders and finally\\ngenerate content in corresponding modalities.\\nAs NExT-GPT encompasses encoding and generation of\\nvarious modalities, training the system from scratch would\\nentail substantial costs. Instead, we take advantage of the ex-\\nisting pre-trained high-performance encoders and decoders,\\nsuch as CLIP (Radford et al., 2021), ImageBind (Girdhar'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='Nevertheless, these MM-LLMs all are subject to the limitation of only perceiving multimodal data, without generating\\ncontent in arbitrary modalities. To achieve LLMs with both multimodal input and output, some thus explore employing LLMs\\nas decision-makers, and utilizing existing off-the-shelf multimodal encoders and decoders as tools to execute multimodal\\ninput and output, such as Visual-ChatGPT (Wu et al., 2023), HuggingGPT (Shen et al., 2023), and AudioGPT (Huang et al.,\\n2023b). As aforementioned, passing messages between modules with pure texts (i.e., LLM textual instruction) under the\\ndiscrete pipeline scheme will inevitably introduce noises. Also lacking comprehensive tuning across the whole system\\nsignificantly limits the efficacy of semantics understanding. Our work takes the mutual benefits of both the above two types,\\ni.e., learning an any-to-any MM-LLM in an end-to-end manner.\\nC. Implementation Details\\nC.1. Detailed Input Projection Layer\\nThrough multimodal encoder, we can obtain patch-level multimodal tokens, denoting as X∗= {x∗\\ni }N ∗\\ni=1, where ∗∈{i, a, v}\\nrepresents image, audio, and video, respectively. For brevity, we eschew modal-specific notation. Differing from the existing\\nworks that directly embed multimodal tokens into LLMs by a linear projection layer, we propose a multi-stage grouping\\nmechanism, where patch-level tokens are grouped into concept-level tokens to facilitate the subsequent cross-modal\\ninteraction. Formally, we apply L grouping stages, and in each stage, we randomly initialize Ml learnable concept tokens\\nCl = {cj}Ml\\nj . Then, we concatenate input features Xl and Cl together and then input them into a transformer layers:\\nˆCl, ˆ\\nXl = Transformer([Cl; Xl]), where X1 = X, and [; ] denotes the concatenation operator. Within l grouping block,\\nwe group the updated Ml concept tokens ˆ\\nXl into Ml+1 new concept tokens ˆ\\nXl+1 based on the feature similarity.\\nSpecifically, we firstly compute a similarity matrix Al between\\nˆCl and\\nˆ\\nXl via a Gumbel-softmax:\\nAl\\n=\\nSoftmax((Norm( ˆCl)Norm( ˆ\\nXl) + G)/τ), where G are i.i.d random samples drawn from the Gumbel(0, 1) distribution,\\nand τ is the learnable significance coefficient to assist to find a more suitable assign boundary. We compute the group to\\nassign a concept token by taking the one-hot operation on the argmax over all the groups. Since the one-hot assignment\\noperation via argmax is not differentiable, we instead use the straight-through trick to compute the assignment matrix as\\nˆ\\nAl = Onehot(Argmax(Al)) + Al −Sg(Al), where Sg(.) is the stop gradient operator. Finally, we integrate the features to\\nupdated concept tokens: Xl+1 = ˆCl + MLP( ˆ\\nAl, ˆ\\nXl). After L stages grouping, we can obtain ML concept tokens XL,\\nwhich are then fed into the LLM for perception and reasoning.\\nC.2. Model Training\\nFor NExT-GPT model training, we consider a three-stage learning process:\\n• Stage-1: Encoding-side Alignment Learning. As discussed in Section §4.1, we bridge the alignment to perform\\nthe caption generation task. The cross-entropy is employed as the loss function. During training, we only keep the\\ninput projection layer trainable while the other part of NExT-GPT is frozen. We employ Adam (Kingma & Ba, 2015)\\noptimizer to update the parameters. This stage can be understood as training a compatible multimodal tokenizer for the\\nfrozen LLM.\\n• Stage-2: Decoding-side Alignment Learning. The output projection layer adopts a transformer-based architecture\\ncharacterized by a hidden size of 512, 4 attention heads, 4 encoder layers, and 4 decoder layers. Additionally, the\\ndropout ratio is set as 0.1. The optimization process for the three output projection layers involves a combination of'),\n",
       " Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='ties, including language, image, video, or audio, from any\\ncombination of input modalities in parallel. Regrettably,\\nCoDi still falls short of achieving human-like deep reason-\\ning of input content, because it can only deliver parallel\\ncross-modal feeding&generation without any reasoning and\\ndecision-marking capabilities.\\nMultimodal Large Language Models\\nLLMs have al-\\nready made a profound impact and revolution on the entire\\nAI community and beyond (OpenAI, 2022a;b), where a\\nseries of open-source LLMs have greatly spurred advance-\\nment and made contributions to the community (Chiang\\net al., 2023; Touvron et al., 2023; Zhu et al., 2023; Zhang\\net al., 2023a). Building on top of these LLMs, significant\\nefforts have been made to extend them to deal with mul-\\ntimodal inputs and tasks, leading to the development of\\nMM-LLMs. On the one hand, most researchers build fun-\\ndamental MM-LLMs by aligning the well-trained encoders\\nof various modalities to the textual feature space of LLMs\\nto perceive other modal inputs (Huang et al., 2023c; Zhu\\net al., 2023; Su et al., 2022; Koh et al., 2023). For example,\\nFlamingo (Alayrac et al., 2022) uses a cross-attention layer\\nto connect a frozen image encoder to the LLMs. BLIP-2\\n(Li et al., 2023c) employs a Q-Former to translate the input\\nimage queries to the LLMs. There are also various similar\\npractices for building MM-LLMs that are able to under-\\nstand video (e.g., Video-Chat (Li et al., 2023d) and Video-\\nLLaMA (Zhang et al., 2023c)), audio (e.g., SpeechGPT\\n(Zhang et al., 2023b)), etc. Profoundly, PandaGPT (Su et al.,\\n2023) achieves a comprehensive understanding of six differ-\\nent modalities simultaneously by integrating the multimodal\\nencoder, i.e., ImageBind (Girdhar et al., 2023).\\nNevertheless, these MM-LLMs are all limited to only per-\\nceiving multimodal data, without the ability to generate\\ncontent in arbitrary modalities. To enable LLMs with both\\nmultimodal input and output, some efforts explore employ-\\ning LLMs as decision-makers, and utilizing existing off-the-\\nshelf multimodal encoders and decoders as tools to execute\\nmultimodal input and output, such as Visual-ChatGPT (Wu\\net al., 2023), HuggingGPT (Shen et al., 2023), and Audio-\\nGPT (Huang et al., 2023b). As aforementioned, passing\\nmessages between modules with pure texts (i.e., LLM tex-\\ntual instruction) under the discrete pipeline scheme will\\ninevitably introduce noises. Also, the lack of comprehen-\\nsive tuning across the whole system significantly limits the\\nefficacy of semantics understanding. Our work takes the\\nmutual benefits of both the above two types, i.e., learning\\nan any-to-any MM-LLM in an end-to-end manner.\\n3. Overall Architecture\\nFigure 1 presents the schematic overview of the NExT-GPT\\nframework, consisting of three main stages: encoding, LLM\\nunderstanding and reasoning, and decoding.\\nMultimodal Encoding Stage\\nFirst, we leverage existing\\nwell-established models to encode inputs of various modali-\\nties. There are a set of alternatives of encoders for different\\nmodalities, e.g., CLIP (Radford et al., 2021), HuBERT (Hsu\\net al., 2021). Here we take advantage of the ImageBind\\n(Girdhar et al., 2023), which is a unified high-performance\\nencoder across six modalities. With ImageBind, we are\\nspared from managing many numbers of heterogeneous\\nmodal encoders. Then, via a projection layer, different input\\n3\\nNExT-GPT: Any-to-Any Multimodal LLM\\nTable 1. Summary of NExT-GPT system configuration. Only 1% of parameters need updating during fine-tuning.\\nEncoder\\nInput Projection\\nLLM\\nOutput Projection\\nDiffusion\\nName\\nParam\\nName\\nParam\\nName\\nParam')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/neutrino/miniconda3/envs/Ml/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It appears that you've provided a snippet from a research paper on the NExT-GPT system, which is an any-to-any multimodal large language model. The text doesn't seem to have a specific question or problem that needs to be answered, but rather provides information about the NExT-GPT framework and its architecture.\n",
      "\n",
      "If you'd like, I can try to summarize the key points from the provided text:\n",
      "\n",
      "* The NExT-GPT system is an any-to-any multimodal large language model that can understand and generate content in various modalities (e.g., text, image, video, audio).\n",
      "* Most existing multimodal large language models (MM-LLMs) are limited to perceiving multimodal data without generating content in arbitrary modalities.\n",
      "* The NExT-GPT system takes the mutual benefits of both MM-LLMs and LLMs as decision-makers, enabling end-to-end learning for any-to-any multimodal tasks.\n",
      "\n",
      "Please let me know if you have a specific question or problem related to this text!\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt = ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])\n",
    "\n",
    "\n",
    "# LLM\n",
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hallucination Grader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is grounded in / supported by a set of facts. \\n \n",
    "    Here are the facts:\n",
    "    \\n ------- \\n\n",
    "    {documents} \n",
    "    \\n ------- \\n\n",
    "    Here is the answer: {generation}\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the answer is grounded in / supported by a set of facts. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"documents\"],\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Grader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'no'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a grader assessing whether an answer is useful to resolve a question. \\n \n",
    "    Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question}\n",
    "    Give a binary score 'yes' or 'no' to indicate whether the answer is useful to resolve a question. \\n\n",
    "    Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question Re-writer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It seems like you want me to rewrite the initial question into a more optimized form for vector store retrieval, but the initial question itself isn\\'t provided.\\n\\nHowever, I can guide you on how to improve a question for better vector store retrieval:\\n\\n1. **Remove Preamble**: If your question starts with phrases like \"I was wondering,\" \"Can someone help me understand,\" or similar, remove them as they don\\'t add value to the query and might reduce its effectiveness in search.\\n\\n2. **Focus on Key Entities**: Identify key entities (people, places, things) that are crucial for understanding the question. These should be included in your revised question.\\n\\n3. **Use Specific Keywords**: Incorporate specific keywords related to the topic or subject of interest. This can help in retrieving more relevant information from a vector store.\\n\\n4. **Simplify and Clarify**: Simplify the language used while maintaining clarity. Avoid ambiguity by making sure what you\\'re asking is clear.\\n\\n5. **Avoid Negations**: If possible, rephrase questions that include negations (e.g., \"What didn\\'t happen\") to positive queries (e.g., \"What happened\").\\n\\n6. **Use Action Verbs**: Incorporate action verbs like \"find,\" \"show,\" or \"compare\" if they\\'re relevant and can help in specifying what you\\'re looking for.\\n\\n7. **Be Concise**: Keep your question concise while ensuring it still captures the essence of what you\\'re asking.\\n\\nGiven these guidelines, let\\'s assume a hypothetical initial question:\\n\\n**Initial Question:** \"I\\'m trying to find information about llm. Can someone explain how it works and its applications?\"\\n\\n**Improved Question:** \"What are the key features and uses of llm?\"\\n\\nThis revised question is more direct, includes specific keywords (\"llm\"), and focuses on the core query without unnecessary preamble or negations.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOllama(model=local_llm, temperature=0)\n",
    "\n",
    "# Prompt\n",
    "re_write_prompt = PromptTemplate(\n",
    "    template=\"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the initial and formulate an improved question. \\n\n",
    "     Here is the initial question: \\n\\n {question}. Improved question with no preamble: \\n \"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "def retrieve(state):\n",
    "    # Retrieve documents\n",
    "\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    # Generate answer\n",
    "\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "\n",
    "    # Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    # Transform the query to produce a better question.\n",
    "\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    # Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    # Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score[\"score\"]\n",
    "\n",
    "    # Check hallucination\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "Node 'retrieve':\n",
      "{'documents': [Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='quality of base models, may produce low-quality or halluci-\\nnated content that could be harmful. Users are cautioned to\\ninterpret results carefully and adhere to licensing rules, with\\ncommercial use prohibited. We prioritize data privacy by\\nfollowing social media platform terms and obtaining user\\nconsent when necessary, ensuring all personal information\\nis anonymized or obfuscated. Additionally, we are vigilant\\nin minimizing bias in dataset collection, striving for a repre-\\nsentative and fair dataset that does not favor or disfavor any\\nparticular group or perspective.\\n9\\nNExT-GPT: Any-to-Any Multimodal LLM\\nReferences\\nAgrawal, H., Anderson, P., Desai, K., Wang, Y., Chen, X.,\\nJain, R., Johnson, M., Batra, D., Parikh, D., and Lee, S.\\nnocaps: novel object captioning at scale. In Proceedings\\nof the ICCV, pp. 8947–8956, 2019.\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Has-\\nson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong,\\nZ., Samangooei, S., Monteiro, M., Menick, J. L.,\\nBorgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh,\\nS., Binkowski, M., Barreira, R., Vinyals, O., Zisserman,\\nA., and Simonyan, K. Flamingo: a visual language model\\nfor few-shot learning. In Proceedings of the NeurIPS,\\n2022.\\nAn, J., Zhang, S., Yang, H., Gupta, S., Huang, J., Luo,\\nJ., and Yin, X. Latent-shift: Latent diffusion with tem-\\nporal shift for efficient text-to-video generation. CoRR,\\nabs/2304.08477, 2023.\\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\\nGould, S., and Zhang, L. Bottom-up and top-down atten-\\ntion for image captioning and visual question answering.\\nIn Proceedings of the CVPR, pp. 6077–6086, 2018.\\nAvrahami, O., Fried, O., and Lischinski, D. Blended la-\\ntent diffusion. ACM Trans. Graph., 42(4):149:1–149:11,\\n2023.\\nBain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen\\nin time: A joint video and image encoder for end-to-end\\nretrieval. In Proceedings of the ICCV, pp. 1708–1718,\\n2021.\\nBashiri, M., Walker, E. Y., Lurz, K., Jagadish, A., Muham-\\nmad, T., Ding, Z., Ding, Z., Tolias, A. S., and Sinz, F. H.\\nA flow-based latent state generative model of neural pop-\\nulation responses to natural images. In Proceedings of\\nthe NeurIPS, pp. 15801–15815, 2021.\\nBrock, A., Donahue, J., and Simonyan, K. Large scale\\nGAN training for high fidelity natural image synthesis.\\nIn Proceedings of the ICLR, 2019.\\nCerspense. Zeroscope: Diffusion-based text-to-video syn-\\nthesis. 2023. URL https://huggingface.co/\\ncerspense.\\nCeylan, D., Huang, C. P., and Mitra, N. J. Pix2video: Video\\nediting using image diffusion. CoRR, abs/2303.12688,\\n2023.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\\nStoica, I., and Xing, E. P. Vicuna: An open-source chatbot\\nimpressing gpt-4 with 902023.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,'),\n",
      "               Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='the user’s instruction, and ‘OUTPUT’ signifies the desired\\nmodel output that conforms to the given instruction. Techni-\\ncally, we leverage LoRA (Hu et al., 2022) to enable a small\\nsubset of parameters within NExT-GPT to be updated con-\\ncurrently with two layers of projection during the IT phase.\\nAs illustrated in Figure 3, when an IT dialogue sample is\\nfed into the system, the LLM reconstructs and generates the\\ntextual content of input (and represents the multimodal con-\\ntent with the multimodal signal tokens). The optimization\\nis imposed based on gold annotations and LLM’s outputs.\\nIn addition to LLM tuning, we also fine-tune the decoding\\nend of NExT-GPT. We align the modal signal tokens’ rep-\\nresentation encoded by the output projection with the gold\\nmultimodal caption representation encoded by the diffu-\\nsion condition encoder. Thereby, the comprehensive tuning\\nprocess brings closer to the goal of faithful and effective\\ninteraction with users.\\n5.2. Instruction Dataset\\nFor the IT of NExT-GPT, we first consider leveraging the\\nwell-established ‘Text’→‘Text+X’ datasets where ‘X’ could\\nbe the image, video, audio, or others, for example, LLaVA-\\n150K (Liu et al., 2023b), and VideoChat (Li et al., 2023d).\\nHowever, these IT datasets are limited to output textual re-\\nsponses from LLMs. In our any-to-any scenario, the target\\nnot only includes the generations of texts, but also the multi-\\nmodal contents, i.e., ‘Text+X’. Thus, we construct the ‘Text’\\n→‘Text+X’ dataset, i.e., text-to-multimodal (namely T2M)\\ndata. Based on the rich volume of ‘X-caption’ pairs from\\nthe existing corpus and benchmarks (Sharma et al., 2018;\\nLin et al., 2014; Bain et al., 2021; Kim et al., 2019), with\\nsome templates, we employ GPT-4 to produce varied textual\\ninstructions to wrap the captions, and result in the dataset.\\nMosIT Dataset\\nCrafting high-quality instructions that\\ncomprehensively cover the desired target behaviors is non-\\ntrivial. We notice that the above IT datasets fail to meet the\\nrequirements for our any-to-any MM-LLM scenario. Firstly,\\nduring a human-machine interaction, users and LLM in-\\nvolve diverse and dynamically changing modalities in their\\ninputs and outputs. Additionally, we allow multi-turn con-\\nversations in the process, and thus the processing and under-\\nstanding of complex user intentions is required. However,\\nthe above two types of datasets lack variable modalities,\\nand also are relatively short in dialogues, failing to mimic\\nreal-world scenarios adequately.\\nTo facilitate the development of any-to-any MM-LLM, we\\npropose a novel Modality-switching Instruction Tuning\\n(MosIT) approach. MosIT not only supports complex\\ncross-modal understanding and reasoning but also enables\\nsophisticated multimodal content generation. In conjunc-\\ntion with MosIT, we manually and meticulously construct\\na high-quality dataset. The MosIT dataset encompasses a\\nwide range of multimodal inputs and outputs, offering the\\nnecessary complexity and variability to facilitate the training\\nof MM-LLMs that can handle diverse user interactions and\\ndeliver the desired responses accurately. Specifically, we de-\\nsign some template dialogue examples between a ‘Human’\\nrole and a ‘Machine’ role, based on which we prompt the\\nGPT-4 to generate more conversations under various scenar-\\nios with more than 100 topics or keywords. The interactions\\nare required to be diversified, e.g., including both straight-\\nforward and implicit requirements by the ‘Human’, and exe-\\ncution of perception, reasoning, suggestion, and planning,\\netc., by the ‘Machine’. And the interactive content should be\\nlogically connected and semantically inherent and complex,\\nwith in-depth reasoning details in each response by the ‘Ma-\\nchine’. Each conversation should include 3-7 turns (i.e., QA'),\n",
      "               Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='pairs), where the ‘Human’-‘Machine’ interactions should\\ninvolve multiple modalities at either the input or output side,\\nand switch the modalities alternately. Whenever multimodal\\ncontents (e.g., image, audio, and video) are present in the\\nconversations, we look for the best-matched contents from\\nthe external resources, including the retrieval systems, e.g.,\\nYoutube, and even AIGC tools, e.g., Stable-XL (Podell et al.,\\n2023), and Midjourney. After human inspections and fil-\\ntering of inappropriate instances, we obtain a total of 5K\\nhigh-quality dialogues. In Table 6 of Appendix §C.4, we\\ncompare the statistics of existing multimodal IT datasets\\nwith our MosIT data in detailed statistics.\\n6\\nNExT-GPT: Any-to-Any Multimodal LLM\\nTable 2. Zero-shot evaluation of image captioning with CIDEr (↑) score on NoCaps (Agrawal et al., 2019), Flickr 30K (Young et al., 2014) and COCO\\n(Karpathy & Fei-Fei, 2017), and image question answering on VQAv2 (Goyal et al., 2017), VizWiz (Gurari et al., 2018) and OKVQA (Marino et al.,\\n2019), and two evaluation-only benchmarks, MMB (Liu et al., 2023c) and SEED (Li et al., 2023a). The best results are marked in bold, and the second\\nones are underlined.\\nModel\\nVersion\\nImage Captioning\\nImage Question Answering\\nComprehensive\\nNoCaps\\nFlickr 30K\\nCOCO\\nVQAv2\\nVizWiz\\nOKVQA\\nMMB\\nSEED\\nInstructBLIP (Dai et al., 2023)\\nVicuna-7B\\n123.1\\n82.4\\n102.2\\n-\\n33.4\\n33.9\\n36.0\\n-\\nLLaVA (Liu et al., 2023b)\\nLLaMA-2-7B-Chat\\n120.7\\n82.7\\n-\\n-\\n-\\n-\\n36.2\\n-\\nmPLUG-Owl (Ye et al., 2023b)\\nLLaMA-7B\\n117.0\\n80.3\\n119.3\\n-\\n39.0\\n-\\n46.6\\n34.0\\nEmu (Sun et al., 2023)\\nLLaMA-7B\\n-\\n-\\n117.7\\n40.0\\n35.4\\n34.7\\n-\\n-\\nDREAMLLM (Dong et al., 2023)\\nVicuna-7B\\n-\\n-\\n115.4\\n56.6\\n45.8\\n44.3\\n49.9\\n-\\nVideo-LLaVA (Lin et al., 2023)\\nVicuna-7B\\n-\\n-\\n-\\n74.7\\n48.1\\n-\\n60.9\\n-\\nNExT-GPT\\nVicuna-7B\\n123.7\\n84.5\\n124.9\\n66.7\\n48.4\\n52.1\\n58.0\\n57.5\\nTable 3. Comparison of video reasoning tasks on MSRVTT (Xu et al., 2016), MSVD-QA and MSRVTT-QA (Xu et al., 2017) and NExTQA (Xiao et al.,\\n2021), and the audio captioning task on AudioCaps (Kim et al., 2019). Scores with ∗means being fine-tuned on the training dataset.\\nModel\\nVersion\\nVideo Captioning\\nVideo Question Answering\\nAudio Captioning\\nMSR-VTT\\nMSVD-QA\\nMSRVTT-QA\\nNExTQA\\nAudioCaps\\nCodi (Tang et al., 2023)\\n-\\n74.4∗\\n-\\n-\\n-\\n78.9∗\\nUIO-2XXL (Lu et al., 2023)\\n6.8B\\n48.8∗\\n41.5\\n52.1\\n-\\n48.9∗\\nVideo-LLaMA (Zhang et al., 2023c)\\nLLaMA-7B\\n-\\n51.6\\n-\\n29.6\\n-\\nVideo-LLaVA (Lin et al., 2023)'),\n",
      "               Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='tasks. In Proceedings of the ECCV, pp. 121–137, 2020.\\nLi, Y., Wang, X., Xiao, J., Ji, W., and Chua, T. Invariant\\ngrounding for video question answering. In Proceedings\\nof the CVPR, pp. 2918–2927, 2022.\\nLi, Y., Zhang, C., Yu, G., Wang, Z., Fu, B., Lin, G., Shen,\\nC., Chen, L., and Wei, Y. Stablellava: Enhanced visual\\ninstruction tuning with synthesized image-dialogue data.\\nCoRR, abs/2308.10253, 2023f.\\nLin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., and Yuan,\\nL. Video-llava: Learning united visual representation\\nby alignment before projection. CoRR, abs/2311.10122,\\n2023.\\nLin, K., Li, L., Lin, C., Ahmed, F., Gan, Z., Liu, Z., Lu, Y.,\\nand Wang, L. Swinbert: End-to-end transformers with\\nsparse attention for video captioning. In Proceedings of\\nthe CVPR, pp. 17928–17937, 2022.\\nLin, T., Maire, M., Belongie, S. J., Hays, J., Perona, P.,\\nRamanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft\\nCOCO: common objects in context. In Fleet, D. J., Pajdla,\\nT., Schiele, B., and Tuytelaars, T. (eds.), Proceedings of\\nthe ECCV, pp. 740–755, 2014.\\nLiu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D. P.,\\nWang, W., and Plumbley, M. D. Audioldm: Text-to-audio\\ngeneration with latent diffusion models. In Proceedings\\nof the ICML, pp. 21450–21474, 2023a.\\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\\ntuning. CoRR, abs/2304.08485, 2023b.\\nLiu, S., Wang, T., Bau, D., Zhu, J., and Torralba, A. Diverse\\nimage generation via self-conditioned gans. In Proceed-\\nings of the CVPR, pp. 14274–14283, 2020.\\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,\\nYuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin,\\nD. Mmbench: Is your multi-modal model an all-around\\nplayer? CoRR, abs/2307.06281, 2023c.\\nLu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten,\\nR., Hoiem, D., and Kembhavi, A. Unified-io 2: Scaling\\nautoregressive multimodal models with vision, language,\\naudio, and action. CoRR, abs/2312.17172, 2023.\\nMaaz, M., Rasheed, H. A., Khan, S. H., and Khan, F. S.\\nVideo-chatgpt: Towards detailed video understanding via\\nlarge vision and language models. CoRR, abs/2306.05424,\\n2023.\\nMarino, K., Rastegari, M., Farhadi, A., and Mottaghi, R.\\nOK-VQA: A visual question answering benchmark re-\\nquiring external knowledge. In Proceedings of the CVPR,\\npp. 3195–3204, 2019.\\nMeng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J., and\\nErmon, S. Sdedit: Guided image synthesis and editing\\nwith stochastic differential equations. In Proceedings of\\nthe ICLR, 2022.\\nMilewski, V. S. J., Moens, M., and Calixto, I. Are scene\\ngraphs good enough to improve image captioning? In\\nProceedings of the AACL, pp. 504–515, 2020.')],\n",
      " 'question': 'Explain the content thoroughly'}\n",
      "\n",
      "---\n",
      "\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT NOT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "Node 'grade_documents':\n",
      "{'documents': [Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='quality of base models, may produce low-quality or halluci-\\nnated content that could be harmful. Users are cautioned to\\ninterpret results carefully and adhere to licensing rules, with\\ncommercial use prohibited. We prioritize data privacy by\\nfollowing social media platform terms and obtaining user\\nconsent when necessary, ensuring all personal information\\nis anonymized or obfuscated. Additionally, we are vigilant\\nin minimizing bias in dataset collection, striving for a repre-\\nsentative and fair dataset that does not favor or disfavor any\\nparticular group or perspective.\\n9\\nNExT-GPT: Any-to-Any Multimodal LLM\\nReferences\\nAgrawal, H., Anderson, P., Desai, K., Wang, Y., Chen, X.,\\nJain, R., Johnson, M., Batra, D., Parikh, D., and Lee, S.\\nnocaps: novel object captioning at scale. In Proceedings\\nof the ICCV, pp. 8947–8956, 2019.\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Has-\\nson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong,\\nZ., Samangooei, S., Monteiro, M., Menick, J. L.,\\nBorgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh,\\nS., Binkowski, M., Barreira, R., Vinyals, O., Zisserman,\\nA., and Simonyan, K. Flamingo: a visual language model\\nfor few-shot learning. In Proceedings of the NeurIPS,\\n2022.\\nAn, J., Zhang, S., Yang, H., Gupta, S., Huang, J., Luo,\\nJ., and Yin, X. Latent-shift: Latent diffusion with tem-\\nporal shift for efficient text-to-video generation. CoRR,\\nabs/2304.08477, 2023.\\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\\nGould, S., and Zhang, L. Bottom-up and top-down atten-\\ntion for image captioning and visual question answering.\\nIn Proceedings of the CVPR, pp. 6077–6086, 2018.\\nAvrahami, O., Fried, O., and Lischinski, D. Blended la-\\ntent diffusion. ACM Trans. Graph., 42(4):149:1–149:11,\\n2023.\\nBain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen\\nin time: A joint video and image encoder for end-to-end\\nretrieval. In Proceedings of the ICCV, pp. 1708–1718,\\n2021.\\nBashiri, M., Walker, E. Y., Lurz, K., Jagadish, A., Muham-\\nmad, T., Ding, Z., Ding, Z., Tolias, A. S., and Sinz, F. H.\\nA flow-based latent state generative model of neural pop-\\nulation responses to natural images. In Proceedings of\\nthe NeurIPS, pp. 15801–15815, 2021.\\nBrock, A., Donahue, J., and Simonyan, K. Large scale\\nGAN training for high fidelity natural image synthesis.\\nIn Proceedings of the ICLR, 2019.\\nCerspense. Zeroscope: Diffusion-based text-to-video syn-\\nthesis. 2023. URL https://huggingface.co/\\ncerspense.\\nCeylan, D., Huang, C. P., and Mitra, N. J. Pix2video: Video\\nediting using image diffusion. CoRR, abs/2303.12688,\\n2023.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\\nStoica, I., and Xing, E. P. Vicuna: An open-source chatbot\\nimpressing gpt-4 with 902023.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,'),\n",
      "               Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='the user’s instruction, and ‘OUTPUT’ signifies the desired\\nmodel output that conforms to the given instruction. Techni-\\ncally, we leverage LoRA (Hu et al., 2022) to enable a small\\nsubset of parameters within NExT-GPT to be updated con-\\ncurrently with two layers of projection during the IT phase.\\nAs illustrated in Figure 3, when an IT dialogue sample is\\nfed into the system, the LLM reconstructs and generates the\\ntextual content of input (and represents the multimodal con-\\ntent with the multimodal signal tokens). The optimization\\nis imposed based on gold annotations and LLM’s outputs.\\nIn addition to LLM tuning, we also fine-tune the decoding\\nend of NExT-GPT. We align the modal signal tokens’ rep-\\nresentation encoded by the output projection with the gold\\nmultimodal caption representation encoded by the diffu-\\nsion condition encoder. Thereby, the comprehensive tuning\\nprocess brings closer to the goal of faithful and effective\\ninteraction with users.\\n5.2. Instruction Dataset\\nFor the IT of NExT-GPT, we first consider leveraging the\\nwell-established ‘Text’→‘Text+X’ datasets where ‘X’ could\\nbe the image, video, audio, or others, for example, LLaVA-\\n150K (Liu et al., 2023b), and VideoChat (Li et al., 2023d).\\nHowever, these IT datasets are limited to output textual re-\\nsponses from LLMs. In our any-to-any scenario, the target\\nnot only includes the generations of texts, but also the multi-\\nmodal contents, i.e., ‘Text+X’. Thus, we construct the ‘Text’\\n→‘Text+X’ dataset, i.e., text-to-multimodal (namely T2M)\\ndata. Based on the rich volume of ‘X-caption’ pairs from\\nthe existing corpus and benchmarks (Sharma et al., 2018;\\nLin et al., 2014; Bain et al., 2021; Kim et al., 2019), with\\nsome templates, we employ GPT-4 to produce varied textual\\ninstructions to wrap the captions, and result in the dataset.\\nMosIT Dataset\\nCrafting high-quality instructions that\\ncomprehensively cover the desired target behaviors is non-\\ntrivial. We notice that the above IT datasets fail to meet the\\nrequirements for our any-to-any MM-LLM scenario. Firstly,\\nduring a human-machine interaction, users and LLM in-\\nvolve diverse and dynamically changing modalities in their\\ninputs and outputs. Additionally, we allow multi-turn con-\\nversations in the process, and thus the processing and under-\\nstanding of complex user intentions is required. However,\\nthe above two types of datasets lack variable modalities,\\nand also are relatively short in dialogues, failing to mimic\\nreal-world scenarios adequately.\\nTo facilitate the development of any-to-any MM-LLM, we\\npropose a novel Modality-switching Instruction Tuning\\n(MosIT) approach. MosIT not only supports complex\\ncross-modal understanding and reasoning but also enables\\nsophisticated multimodal content generation. In conjunc-\\ntion with MosIT, we manually and meticulously construct\\na high-quality dataset. The MosIT dataset encompasses a\\nwide range of multimodal inputs and outputs, offering the\\nnecessary complexity and variability to facilitate the training\\nof MM-LLMs that can handle diverse user interactions and\\ndeliver the desired responses accurately. Specifically, we de-\\nsign some template dialogue examples between a ‘Human’\\nrole and a ‘Machine’ role, based on which we prompt the\\nGPT-4 to generate more conversations under various scenar-\\nios with more than 100 topics or keywords. The interactions\\nare required to be diversified, e.g., including both straight-\\nforward and implicit requirements by the ‘Human’, and exe-\\ncution of perception, reasoning, suggestion, and planning,\\netc., by the ‘Machine’. And the interactive content should be\\nlogically connected and semantically inherent and complex,\\nwith in-depth reasoning details in each response by the ‘Ma-\\nchine’. Each conversation should include 3-7 turns (i.e., QA'),\n",
      "               Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='pairs), where the ‘Human’-‘Machine’ interactions should\\ninvolve multiple modalities at either the input or output side,\\nand switch the modalities alternately. Whenever multimodal\\ncontents (e.g., image, audio, and video) are present in the\\nconversations, we look for the best-matched contents from\\nthe external resources, including the retrieval systems, e.g.,\\nYoutube, and even AIGC tools, e.g., Stable-XL (Podell et al.,\\n2023), and Midjourney. After human inspections and fil-\\ntering of inappropriate instances, we obtain a total of 5K\\nhigh-quality dialogues. In Table 6 of Appendix §C.4, we\\ncompare the statistics of existing multimodal IT datasets\\nwith our MosIT data in detailed statistics.\\n6\\nNExT-GPT: Any-to-Any Multimodal LLM\\nTable 2. Zero-shot evaluation of image captioning with CIDEr (↑) score on NoCaps (Agrawal et al., 2019), Flickr 30K (Young et al., 2014) and COCO\\n(Karpathy & Fei-Fei, 2017), and image question answering on VQAv2 (Goyal et al., 2017), VizWiz (Gurari et al., 2018) and OKVQA (Marino et al.,\\n2019), and two evaluation-only benchmarks, MMB (Liu et al., 2023c) and SEED (Li et al., 2023a). The best results are marked in bold, and the second\\nones are underlined.\\nModel\\nVersion\\nImage Captioning\\nImage Question Answering\\nComprehensive\\nNoCaps\\nFlickr 30K\\nCOCO\\nVQAv2\\nVizWiz\\nOKVQA\\nMMB\\nSEED\\nInstructBLIP (Dai et al., 2023)\\nVicuna-7B\\n123.1\\n82.4\\n102.2\\n-\\n33.4\\n33.9\\n36.0\\n-\\nLLaVA (Liu et al., 2023b)\\nLLaMA-2-7B-Chat\\n120.7\\n82.7\\n-\\n-\\n-\\n-\\n36.2\\n-\\nmPLUG-Owl (Ye et al., 2023b)\\nLLaMA-7B\\n117.0\\n80.3\\n119.3\\n-\\n39.0\\n-\\n46.6\\n34.0\\nEmu (Sun et al., 2023)\\nLLaMA-7B\\n-\\n-\\n117.7\\n40.0\\n35.4\\n34.7\\n-\\n-\\nDREAMLLM (Dong et al., 2023)\\nVicuna-7B\\n-\\n-\\n115.4\\n56.6\\n45.8\\n44.3\\n49.9\\n-\\nVideo-LLaVA (Lin et al., 2023)\\nVicuna-7B\\n-\\n-\\n-\\n74.7\\n48.1\\n-\\n60.9\\n-\\nNExT-GPT\\nVicuna-7B\\n123.7\\n84.5\\n124.9\\n66.7\\n48.4\\n52.1\\n58.0\\n57.5\\nTable 3. Comparison of video reasoning tasks on MSRVTT (Xu et al., 2016), MSVD-QA and MSRVTT-QA (Xu et al., 2017) and NExTQA (Xiao et al.,\\n2021), and the audio captioning task on AudioCaps (Kim et al., 2019). Scores with ∗means being fine-tuned on the training dataset.\\nModel\\nVersion\\nVideo Captioning\\nVideo Question Answering\\nAudio Captioning\\nMSR-VTT\\nMSVD-QA\\nMSRVTT-QA\\nNExTQA\\nAudioCaps\\nCodi (Tang et al., 2023)\\n-\\n74.4∗\\n-\\n-\\n-\\n78.9∗\\nUIO-2XXL (Lu et al., 2023)\\n6.8B\\n48.8∗\\n41.5\\n52.1\\n-\\n48.9∗\\nVideo-LLaMA (Zhang et al., 2023c)\\nLLaMA-7B\\n-\\n51.6\\n-\\n29.6\\n-\\nVideo-LLaVA (Lin et al., 2023)')],\n",
      " 'question': 'Explain the content thoroughly'}\n",
      "\n",
      "---\n",
      "\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "Node 'generate':\n",
      "{'documents': [Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='quality of base models, may produce low-quality or halluci-\\nnated content that could be harmful. Users are cautioned to\\ninterpret results carefully and adhere to licensing rules, with\\ncommercial use prohibited. We prioritize data privacy by\\nfollowing social media platform terms and obtaining user\\nconsent when necessary, ensuring all personal information\\nis anonymized or obfuscated. Additionally, we are vigilant\\nin minimizing bias in dataset collection, striving for a repre-\\nsentative and fair dataset that does not favor or disfavor any\\nparticular group or perspective.\\n9\\nNExT-GPT: Any-to-Any Multimodal LLM\\nReferences\\nAgrawal, H., Anderson, P., Desai, K., Wang, Y., Chen, X.,\\nJain, R., Johnson, M., Batra, D., Parikh, D., and Lee, S.\\nnocaps: novel object captioning at scale. In Proceedings\\nof the ICCV, pp. 8947–8956, 2019.\\nAlayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Has-\\nson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\\nM., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong,\\nZ., Samangooei, S., Monteiro, M., Menick, J. L.,\\nBorgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh,\\nS., Binkowski, M., Barreira, R., Vinyals, O., Zisserman,\\nA., and Simonyan, K. Flamingo: a visual language model\\nfor few-shot learning. In Proceedings of the NeurIPS,\\n2022.\\nAn, J., Zhang, S., Yang, H., Gupta, S., Huang, J., Luo,\\nJ., and Yin, X. Latent-shift: Latent diffusion with tem-\\nporal shift for efficient text-to-video generation. CoRR,\\nabs/2304.08477, 2023.\\nAnderson, P., He, X., Buehler, C., Teney, D., Johnson, M.,\\nGould, S., and Zhang, L. Bottom-up and top-down atten-\\ntion for image captioning and visual question answering.\\nIn Proceedings of the CVPR, pp. 6077–6086, 2018.\\nAvrahami, O., Fried, O., and Lischinski, D. Blended la-\\ntent diffusion. ACM Trans. Graph., 42(4):149:1–149:11,\\n2023.\\nBain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen\\nin time: A joint video and image encoder for end-to-end\\nretrieval. In Proceedings of the ICCV, pp. 1708–1718,\\n2021.\\nBashiri, M., Walker, E. Y., Lurz, K., Jagadish, A., Muham-\\nmad, T., Ding, Z., Ding, Z., Tolias, A. S., and Sinz, F. H.\\nA flow-based latent state generative model of neural pop-\\nulation responses to natural images. In Proceedings of\\nthe NeurIPS, pp. 15801–15815, 2021.\\nBrock, A., Donahue, J., and Simonyan, K. Large scale\\nGAN training for high fidelity natural image synthesis.\\nIn Proceedings of the ICLR, 2019.\\nCerspense. Zeroscope: Diffusion-based text-to-video syn-\\nthesis. 2023. URL https://huggingface.co/\\ncerspense.\\nCeylan, D., Huang, C. P., and Mitra, N. J. Pix2video: Video\\nediting using image diffusion. CoRR, abs/2303.12688,\\n2023.\\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\\nStoica, I., and Xing, E. P. Vicuna: An open-source chatbot\\nimpressing gpt-4 with 902023.\\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,'),\n",
      "               Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='the user’s instruction, and ‘OUTPUT’ signifies the desired\\nmodel output that conforms to the given instruction. Techni-\\ncally, we leverage LoRA (Hu et al., 2022) to enable a small\\nsubset of parameters within NExT-GPT to be updated con-\\ncurrently with two layers of projection during the IT phase.\\nAs illustrated in Figure 3, when an IT dialogue sample is\\nfed into the system, the LLM reconstructs and generates the\\ntextual content of input (and represents the multimodal con-\\ntent with the multimodal signal tokens). The optimization\\nis imposed based on gold annotations and LLM’s outputs.\\nIn addition to LLM tuning, we also fine-tune the decoding\\nend of NExT-GPT. We align the modal signal tokens’ rep-\\nresentation encoded by the output projection with the gold\\nmultimodal caption representation encoded by the diffu-\\nsion condition encoder. Thereby, the comprehensive tuning\\nprocess brings closer to the goal of faithful and effective\\ninteraction with users.\\n5.2. Instruction Dataset\\nFor the IT of NExT-GPT, we first consider leveraging the\\nwell-established ‘Text’→‘Text+X’ datasets where ‘X’ could\\nbe the image, video, audio, or others, for example, LLaVA-\\n150K (Liu et al., 2023b), and VideoChat (Li et al., 2023d).\\nHowever, these IT datasets are limited to output textual re-\\nsponses from LLMs. In our any-to-any scenario, the target\\nnot only includes the generations of texts, but also the multi-\\nmodal contents, i.e., ‘Text+X’. Thus, we construct the ‘Text’\\n→‘Text+X’ dataset, i.e., text-to-multimodal (namely T2M)\\ndata. Based on the rich volume of ‘X-caption’ pairs from\\nthe existing corpus and benchmarks (Sharma et al., 2018;\\nLin et al., 2014; Bain et al., 2021; Kim et al., 2019), with\\nsome templates, we employ GPT-4 to produce varied textual\\ninstructions to wrap the captions, and result in the dataset.\\nMosIT Dataset\\nCrafting high-quality instructions that\\ncomprehensively cover the desired target behaviors is non-\\ntrivial. We notice that the above IT datasets fail to meet the\\nrequirements for our any-to-any MM-LLM scenario. Firstly,\\nduring a human-machine interaction, users and LLM in-\\nvolve diverse and dynamically changing modalities in their\\ninputs and outputs. Additionally, we allow multi-turn con-\\nversations in the process, and thus the processing and under-\\nstanding of complex user intentions is required. However,\\nthe above two types of datasets lack variable modalities,\\nand also are relatively short in dialogues, failing to mimic\\nreal-world scenarios adequately.\\nTo facilitate the development of any-to-any MM-LLM, we\\npropose a novel Modality-switching Instruction Tuning\\n(MosIT) approach. MosIT not only supports complex\\ncross-modal understanding and reasoning but also enables\\nsophisticated multimodal content generation. In conjunc-\\ntion with MosIT, we manually and meticulously construct\\na high-quality dataset. The MosIT dataset encompasses a\\nwide range of multimodal inputs and outputs, offering the\\nnecessary complexity and variability to facilitate the training\\nof MM-LLMs that can handle diverse user interactions and\\ndeliver the desired responses accurately. Specifically, we de-\\nsign some template dialogue examples between a ‘Human’\\nrole and a ‘Machine’ role, based on which we prompt the\\nGPT-4 to generate more conversations under various scenar-\\nios with more than 100 topics or keywords. The interactions\\nare required to be diversified, e.g., including both straight-\\nforward and implicit requirements by the ‘Human’, and exe-\\ncution of perception, reasoning, suggestion, and planning,\\netc., by the ‘Machine’. And the interactive content should be\\nlogically connected and semantically inherent and complex,\\nwith in-depth reasoning details in each response by the ‘Ma-\\nchine’. Each conversation should include 3-7 turns (i.e., QA'),\n",
      "               Document(metadata={'source': '/mnt/Main Drive/Codes/LLM Agents/2309.05519v3.pdf'}, page_content='pairs), where the ‘Human’-‘Machine’ interactions should\\ninvolve multiple modalities at either the input or output side,\\nand switch the modalities alternately. Whenever multimodal\\ncontents (e.g., image, audio, and video) are present in the\\nconversations, we look for the best-matched contents from\\nthe external resources, including the retrieval systems, e.g.,\\nYoutube, and even AIGC tools, e.g., Stable-XL (Podell et al.,\\n2023), and Midjourney. After human inspections and fil-\\ntering of inappropriate instances, we obtain a total of 5K\\nhigh-quality dialogues. In Table 6 of Appendix §C.4, we\\ncompare the statistics of existing multimodal IT datasets\\nwith our MosIT data in detailed statistics.\\n6\\nNExT-GPT: Any-to-Any Multimodal LLM\\nTable 2. Zero-shot evaluation of image captioning with CIDEr (↑) score on NoCaps (Agrawal et al., 2019), Flickr 30K (Young et al., 2014) and COCO\\n(Karpathy & Fei-Fei, 2017), and image question answering on VQAv2 (Goyal et al., 2017), VizWiz (Gurari et al., 2018) and OKVQA (Marino et al.,\\n2019), and two evaluation-only benchmarks, MMB (Liu et al., 2023c) and SEED (Li et al., 2023a). The best results are marked in bold, and the second\\nones are underlined.\\nModel\\nVersion\\nImage Captioning\\nImage Question Answering\\nComprehensive\\nNoCaps\\nFlickr 30K\\nCOCO\\nVQAv2\\nVizWiz\\nOKVQA\\nMMB\\nSEED\\nInstructBLIP (Dai et al., 2023)\\nVicuna-7B\\n123.1\\n82.4\\n102.2\\n-\\n33.4\\n33.9\\n36.0\\n-\\nLLaVA (Liu et al., 2023b)\\nLLaMA-2-7B-Chat\\n120.7\\n82.7\\n-\\n-\\n-\\n-\\n36.2\\n-\\nmPLUG-Owl (Ye et al., 2023b)\\nLLaMA-7B\\n117.0\\n80.3\\n119.3\\n-\\n39.0\\n-\\n46.6\\n34.0\\nEmu (Sun et al., 2023)\\nLLaMA-7B\\n-\\n-\\n117.7\\n40.0\\n35.4\\n34.7\\n-\\n-\\nDREAMLLM (Dong et al., 2023)\\nVicuna-7B\\n-\\n-\\n115.4\\n56.6\\n45.8\\n44.3\\n49.9\\n-\\nVideo-LLaVA (Lin et al., 2023)\\nVicuna-7B\\n-\\n-\\n-\\n74.7\\n48.1\\n-\\n60.9\\n-\\nNExT-GPT\\nVicuna-7B\\n123.7\\n84.5\\n124.9\\n66.7\\n48.4\\n52.1\\n58.0\\n57.5\\nTable 3. Comparison of video reasoning tasks on MSRVTT (Xu et al., 2016), MSVD-QA and MSRVTT-QA (Xu et al., 2017) and NExTQA (Xiao et al.,\\n2021), and the audio captioning task on AudioCaps (Kim et al., 2019). Scores with ∗means being fine-tuned on the training dataset.\\nModel\\nVersion\\nVideo Captioning\\nVideo Question Answering\\nAudio Captioning\\nMSR-VTT\\nMSVD-QA\\nMSRVTT-QA\\nNExTQA\\nAudioCaps\\nCodi (Tang et al., 2023)\\n-\\n74.4∗\\n-\\n-\\n-\\n78.9∗\\nUIO-2XXL (Lu et al., 2023)\\n6.8B\\n48.8∗\\n41.5\\n52.1\\n-\\n48.9∗\\nVideo-LLaMA (Zhang et al., 2023c)\\nLLaMA-7B\\n-\\n51.6\\n-\\n29.6\\n-\\nVideo-LLaVA (Lin et al., 2023)')],\n",
      " 'generation': \"It appears that you've provided a large block of text from a \"\n",
      "               'research paper on multimodal language models, specifically '\n",
      "               \"NExT-GPT. I'll do my best to summarize the key points and \"\n",
      "               'answer any specific questions you may have.\\n'\n",
      "               '\\n'\n",
      "               '**Summary**\\n'\n",
      "               '\\n'\n",
      "               'The paper presents NExT-GPT, an any-to-any multimodal language '\n",
      "               'model that can handle various tasks such as image captioning, '\n",
      "               'video reasoning, audio captioning, and more. The authors '\n",
      "               'introduce a new dataset called MosIT (Multimodal Instructed '\n",
      "               'Task) with 5K high-quality dialogues involving multiple '\n",
      "               'modalities.\\n'\n",
      "               '\\n'\n",
      "               '**Key Findings**\\n'\n",
      "               '\\n'\n",
      "               '* NExT-GPT achieves state-of-the-art results on various '\n",
      "               'multimodal tasks, including image captioning, video reasoning, '\n",
      "               'and audio captioning.\\n'\n",
      "               '* The model is trained on a large-scale dataset and fine-tuned '\n",
      "               'on specific tasks to achieve better performance.\\n'\n",
      "               '* The authors compare the performance of NExT-GPT with other '\n",
      "               'state-of-the-art models on several benchmarks.\\n'\n",
      "               '\\n'\n",
      "               '**Specific Questions**\\n'\n",
      "               '\\n'\n",
      "               'If you have any specific questions about the paper or would '\n",
      "               'like me to clarify certain points, please feel free to ask!',\n",
      " 'question': 'Explain the content thoroughly'}\n",
      "\n",
      "---\n",
      "\n",
      "(\"It appears that you've provided a large block of text from a research paper \"\n",
      " \"on multimodal language models, specifically NExT-GPT. I'll do my best to \"\n",
      " 'summarize the key points and answer any specific questions you may have.\\n'\n",
      " '\\n'\n",
      " '**Summary**\\n'\n",
      " '\\n'\n",
      " 'The paper presents NExT-GPT, an any-to-any multimodal language model that '\n",
      " 'can handle various tasks such as image captioning, video reasoning, audio '\n",
      " 'captioning, and more. The authors introduce a new dataset called MosIT '\n",
      " '(Multimodal Instructed Task) with 5K high-quality dialogues involving '\n",
      " 'multiple modalities.\\n'\n",
      " '\\n'\n",
      " '**Key Findings**\\n'\n",
      " '\\n'\n",
      " '* NExT-GPT achieves state-of-the-art results on various multimodal tasks, '\n",
      " 'including image captioning, video reasoning, and audio captioning.\\n'\n",
      " '* The model is trained on a large-scale dataset and fine-tuned on specific '\n",
      " 'tasks to achieve better performance.\\n'\n",
      " '* The authors compare the performance of NExT-GPT with other '\n",
      " 'state-of-the-art models on several benchmarks.\\n'\n",
      " '\\n'\n",
      " '**Specific Questions**\\n'\n",
      " '\\n'\n",
      " 'If you have any specific questions about the paper or would like me to '\n",
      " 'clarify certain points, please feel free to ask!')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"Explain the content thoroughly\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        print(f\"Node '{key}':\")\n",
    "        pprint(value)\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
